{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train.csv', encoding='cp949')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## 변수들을 영문명으로 변경\n",
    "cols = ['num', 'date_time', 'power', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "train.columns = cols\n",
    "\n",
    "# 시간 관련 변수들 생성\n",
    "date = pd.to_datetime(train.date_time)\n",
    "train['hour'] = date.dt.hour\n",
    "train['day'] = date.dt.weekday\n",
    "train['month'] = date.dt.month\n",
    "train['week'] = date.dt.weekofyear\n",
    "\n",
    "\n",
    "## 건물별, 요일별, 시간별 발전량 평균 넣어주기\n",
    "\n",
    "power_mean = pd.pivot_table(train, values = 'power', index = ['num', 'hour', 'day'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['day_hour_mean'] = train.progress_apply(lambda x : power_mean.loc[(power_mean.num == x['num']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "\n",
    "## 건물별 시간별 발전량 평균 넣어주기\n",
    " \n",
    "power_hour_mean = pd.pivot_table(train, values = 'power', index = ['num', 'hour'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_mean'] = train.progress_apply(lambda x : power_hour_mean.loc[(power_hour_mean.num == x['num']) & (power_hour_mean.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "\n",
    "## 건물별 시간별 발전량 표준편차 넣어주기\n",
    "\n",
    "power_hour_std = pd.pivot_table(train, values = 'power', index = ['num', 'hour'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_std'] = train.progress_apply(lambda x : power_hour_std.loc[(power_hour_std.num == x['num']) & (power_hour_std.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "### 공휴일 변수 추가\n",
    "train['holiday'] = train.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n",
    "train.loc[('2020-08-17'<=train.date_time)&(train.date_time<'2020-08-18'), 'holiday'] = 1\n",
    "\n",
    "## https://dacon.io/competitions/official/235680/codeshare/2366?page=1&dtype=recent\n",
    "train['sin_time'] = np.sin(2*np.pi*train.hour/24)\n",
    "train['cos_time'] = np.cos(2*np.pi*train.hour/24)\n",
    "\n",
    "## https://dacon.io/competitions/official/235736/codeshare/2743?page=1&dtype=recent\n",
    "train['THI'] = 9/5*train['temp'] - 0.55*(1-train['hum']/100)*(9/5*train['hum']-26)+32\n",
    "\n",
    "def CDH(xs):\n",
    "    ys = []\n",
    "    for i in range(len(xs)):\n",
    "        if i < 11:\n",
    "            ys.append(np.sum(xs[:(i+1)]-26))\n",
    "        else:\n",
    "            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "    return np.array(ys)\n",
    "\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = train[train['num'] == num]\n",
    "    cdh = CDH(temp['temp'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "train['CDH'] = cdhs\n",
    "\n",
    "train.drop(['non_elec','solar','hour'], axis = 1, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the preprocessed data\n",
    "train.to_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA/SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CSV 파일에서 전체 데이터 불러오기\n",
    "df = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv', parse_dates=['date_time'], index_col='date_time')\n",
    "\n",
    "# 건물 번호 리스트 생성\n",
    "building_nums = df['num'].unique()\n",
    "\n",
    "# 각 건물에 대해 ARIMA 모델 적용 및 예측 시각화\n",
    "for num in building_nums:\n",
    "    # 건물별 데이터 추출\n",
    "    building_data = df[df['num'] == num]['power']\n",
    "    \n",
    "    # 데이터 분할\n",
    "    train_size = int(len(building_data) * 0.8)\n",
    "    train, test = building_data.iloc[:train_size], building_data.iloc[train_size:]\n",
    "    \n",
    "    # ARIMA 모델 학습 및 예측\n",
    "    model = ARIMA(train, order=(1, 1, 1))\n",
    "    arima_fit = model.fit()\n",
    "    pred = arima_fit.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "    \n",
    "    # 평가 및 시각화\n",
    "    mse = mean_squared_error(test, pred)\n",
    "    print(f\"Building {num} - Mean Squared Error: {mse}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Test')\n",
    "    plt.plot(test.index, pred, label='Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Building {num} - ARIMA Model Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CSV 파일에서 전체 데이터 불러오기\n",
    "df = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv', parse_dates=['date_time'], index_col='date_time')\n",
    "\n",
    "# 건물 번호 리스트 생성\n",
    "building_nums = df['num'].unique()\n",
    "\n",
    "# 각 건물에 대해 SARIMA 모델 적용 및 예측 시각화\n",
    "for num in building_nums:\n",
    "    # 건물별 데이터 추출\n",
    "    building_data = df[df['num'] == num]['power']\n",
    "    \n",
    "    # 데이터 분할\n",
    "    train_size = int(len(building_data) * 0.8)\n",
    "    train, test = building_data.iloc[:train_size], building_data.iloc[train_size:]\n",
    "    \n",
    "    # SARIMA 모델 학습 및 예측\n",
    "    model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
    "    sarima_fit = model.fit(disp=False)\n",
    "    pred = sarima_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "    \n",
    "    # 평가 및 시각화\n",
    "    mse = mean_squared_error(test, pred)\n",
    "    print(f\"Building {num} - Mean Squared Error: {mse}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Test')\n",
    "    plt.plot(test.index, pred, label='Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Building {num} - SARIMA Model Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF/PADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# 데이터 로드\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv',encoding='euc-kr', parse_dates=['date_time'])\n",
    "test_df = pd.read_csv('test.csv', encoding='euc-kr', parse_dates=['date_time'])\n",
    "sub = pd.read_csv('sample_submission.csv', encoding='euc-kr')\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "data=train_df\n",
    "\n",
    "# 'datetime' 열을 datetime 타입으로 변환\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "# 'datetime'을 인덱스로 설정\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# 시계열 데이터를 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['target'], label='Energy Usage (kWh)')\n",
    "plt.title('Energy Usage Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Energy Usage (kWh)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ADF 테스트 함수\n",
    "def adf_test(series):\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "\n",
    "# 'target' 열에 대해 ADF 테스트 수행\n",
    "adf_test(data['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 빌딩 데이터 선택\n",
    "building_num = 1\n",
    "df = data[data['num'] == building_num]\n",
    "\n",
    "# ACF 및 PACF 플롯 생성\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(df['target'], ax=plt.gca())\n",
    "plt.title(f'ACF for Building {building_num}')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(df['target'], method='ywm', ax=plt.gca())\n",
    "plt.title(f'PACF for Building {building_num}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# ADF 테스트 함수\n",
    "def adf_test(series):\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "\n",
    "# 'target' 열에 대해 ADF 테스트 수행\n",
    "adf_test(data['target'])\n",
    "\n",
    "# ACF 및 PACF 플롯 생성\n",
    "num_buildings = 60\n",
    "fig, axes = plt.subplots(num_buildings, 2, figsize=(15, 3*num_buildings))\n",
    "\n",
    "for num in range(1, num_buildings + 1):\n",
    "    df = train_df[train_df.num == num]\n",
    "    \n",
    "    # ACF plot\n",
    "    plot_acf(df['target'], ax=axes[num-1, 0])\n",
    "    axes[num-1, 0].set_title(f'ACF for Building {num}')\n",
    "    \n",
    "    # PACF plot\n",
    "    plot_pacf(df['target'], method='ywm', ax=axes[num-1, 1])\n",
    "    axes[num-1, 1].set_title(f'PACF for Building {num}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  👌군집화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\train.csv', encoding='euc|-kr', parse_dates=['date_time'])\n",
    "test_df = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\electricity_data\\test.csv', encoding='euc-kr', parse_dates=['date_time'])\n",
    "sub = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\sample_submission.csv', encoding='euc-kr')\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "\n",
    "# 데이터 형태 및 개수, 결측치 확인\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 날짜형식으로 바꿈\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "\n",
    "# 시간 열 추가\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "\n",
    "hourly_mean = pd.DataFrame()\n",
    "\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "train_df['weekend'] = train_df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# 주중 데이터 필터링 (주중은 weekday가 0부터 4까지인 날짜)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'datetime'을 datetime으로 변환하고, 시간 추출\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "\n",
    "# 주중 데이터 필터링 (주중은 weekday가 0부터 4까지인 날짜)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n",
    "\n",
    "# 각 건물 별, 시간대 별 평균 에너지 소비량 계산\n",
    "hourly_consumption = weekday_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "\n",
    "# 평균 에너지 소비량 데이터를 NumPy 배열로 변환\n",
    "feature_matrix = hourly_consumption.fillna(0).values\n",
    "\n",
    "# 엘보우 방법을 사용하여 최적의 클러스터 수 찾기\n",
    "sse = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)  # 2부터 10까지의 클러스터 수를 시도\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(feature_matrix, kmeans.labels_))\n",
    "\n",
    "# # 엘보우 방법 그래프\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_range, sse, marker='o')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Sum of squared distances (SSE)')\n",
    "# plt.show()\n",
    "\n",
    "# # 실루엣 계수 그래프\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_range, silhouette_scores, marker='o')\n",
    "# plt.title('Silhouette Scores For Optimal k')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.show()\n",
    "\n",
    "# KMeans 군집화 수행 (최적의 k 선택 후, 예: 6)\n",
    "optimal_k = 6  # 엘보우 방법과 실루엣 계수를 통해 결정된 최적의 클러스터 수\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "clusters = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "# 클러스터 결과를 보기 좋게 출력\n",
    "clustered_buildings = {}\n",
    "for building_idx, cluster_label in enumerate(clusters):\n",
    "    if cluster_label not in clustered_buildings:\n",
    "        clustered_buildings[cluster_label] = []\n",
    "    clustered_buildings[cluster_label].append(building_idx + 1)  \n",
    "\n",
    "# 클러스터 결과 출력\n",
    "for cluster, buildings in clustered_buildings.items():\n",
    "    print(f\"클러스터 {cluster + 1}: 빌딩 {', '.join(map(str, buildings))}\")\n",
    "\n",
    "# # 시각화: 클러스터별 평균 에너지 소비 패턴\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# for cluster_num in range(optimal_k):\n",
    "#     cluster_data = hourly_consumption.iloc[np.where(clusters == cluster_num)].mean(axis=0)\n",
    "    \n",
    "#     plt.plot(cluster_data.index, cluster_data.values, label=f'Cluster {cluster_num + 1}')\n",
    "\n",
    "# plt.title('Mean Hourly Energy Consumption by Cluster (Weekdays)')\n",
    "# plt.xlabel('Hour of Day')\n",
    "# plt.ylabel('Mean Energy Consumption')\n",
    "# plt.legend(title='Cluster')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 변환 및 시간 추출\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "\n",
    "# 주중 데이터 필터링 (주중은 weekday가 0부터 4까지인 날짜)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n",
    "# 주말 데이터 필터링 (주말은 weekday가 5부터 6까지인 날짜)\n",
    "weekend_data = train_df[train_df['weekday'].isin([5, 6])]\n",
    "\n",
    "# 각 건물 별, 시간대 별 평균 에너지 소비량 계산 (주중)\n",
    "weekday_hourly_consumption = weekday_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "# 각 건물 별, 시간대 별 평균 에너지 소비량 계산 (주말)\n",
    "weekend_hourly_consumption = weekend_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "\n",
    "# 평균 에너지 소비량 데이터를 NumPy 배열로 변환 (주중, 주말)\n",
    "weekday_feature_matrix = weekday_hourly_consumption.fillna(0).values\n",
    "weekend_feature_matrix = weekend_hourly_consumption.fillna(0).values\n",
    "\n",
    "# KMeans 군집화 수행 (주중: 3 클러스터, 주말: 4 클러스터)\n",
    "optimal_k_weekday = 3\n",
    "optimal_k_weekend = 4\n",
    "\n",
    "kmeans_weekday = KMeans(n_clusters=optimal_k_weekday, random_state=0)\n",
    "weekday_clusters = kmeans_weekday.fit_predict(weekday_feature_matrix)\n",
    "\n",
    "kmeans_weekend = KMeans(n_clusters=optimal_k_weekend, random_state=0)\n",
    "weekend_clusters = kmeans_weekend.fit_predict(weekend_feature_matrix)\n",
    "\n",
    "def print_clusters(clusters, labels, title):\n",
    "    clustered_buildings = {}\n",
    "    for building_idx, cluster_label in enumerate(clusters):\n",
    "        if labels[cluster_label] not in clustered_buildings:\n",
    "            clustered_buildings[labels[cluster_label]] = []\n",
    "        clustered_buildings[labels[cluster_label]].append(building_idx + 1)\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    for cluster, buildings in clustered_buildings.items():\n",
    "        print(f\"클러스터 {cluster}: 빌딩 {', '.join(map(str, buildings))}\")\n",
    "\n",
    "# 주중 클러스터 결과 출력 (1, 2, 3)\n",
    "weekday_labels = ['1', '2', '3']\n",
    "print_clusters(weekday_clusters, weekday_labels, \"Weekday Clusters\")\n",
    "\n",
    "# 주말 클러스터 결과 출력 (a, b, c, d)\n",
    "weekend_labels = ['a', 'b', 'c', 'd']\n",
    "print_clusters(weekend_clusters, weekend_labels, \"Weekend Clusters\")\n",
    "\n",
    "# 클러스터 교차표 작성\n",
    "cross_tab = pd.crosstab(\n",
    "    pd.Series(weekday_clusters, name='Weekday Cluster').map({0: '1', 1: '2', 2: '3'}),\n",
    "    pd.Series(weekend_clusters, name='Weekend Cluster').map({0: 'a', 1: 'b', 2: 'c', 3: 'd'})\n",
    ")\n",
    "\n",
    "# 보기 좋게 데이터프레임으로 출력\n",
    "cross_tab_df = cross_tab.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# 스타일 적용하여 보기 좋게 출력\n",
    "def style_crosstab(df):\n",
    "    styled_df = df.style.set_table_styles(\n",
    "        [{'selector': 'thead th', 'props': [('background-color', '#f7f7f9'), ('color', 'black')]}]\n",
    "    ).set_properties(**{\n",
    "        'background-color': '#e8e8e8', \n",
    "        'color': 'black', \n",
    "        'border-color': 'white',\n",
    "        'text-align': 'center'\n",
    "    }).set_caption('Cross Tabulation of Weekday and Weekend Clusters').format(na_rep='').hide(axis=\"index\")\n",
    "    return styled_df\n",
    "\n",
    "styled_crosstab = style_crosstab(cross_tab_df)\n",
    "display(styled_crosstab)\n",
    "\n",
    "# # 시각화: 클러스터별 평균 에너지 소비 패턴 (주중, 주말)\n",
    "# def plot_cluster_patterns(feature_matrix, clusters, optimal_k, labels, title):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     for cluster_num in range(optimal_k):\n",
    "#         cluster_data = pd.DataFrame(feature_matrix).iloc[np.where(clusters == cluster_num)].mean(axis=0)\n",
    "#         plt.plot(cluster_data.index, cluster_data.values, label=f'Cluster {labels[cluster_num]}')\n",
    "#     plt.title(f'Mean Hourly Energy Consumption by Cluster ({title})')\n",
    "#     plt.xlabel('Hour of Day')\n",
    "#     plt.ylabel('Mean Energy Consumption')\n",
    "#     plt.legend(title='Cluster')\n",
    "#     # plt.show()\n",
    "\n",
    "# plot_cluster_patterns(weekday_feature_matrix, weekday_clusters, optimal_k_weekday, weekday_labels, \"Weekday\")\n",
    "# plot_cluster_patterns(weekend_feature_matrix, weekend_clusters, optimal_k_weekend, weekend_labels, \"Weekend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 주중 클러스터 레이블과 주말 클러스터 레이블을 각각의 열로 추가\n",
    "train_df['weekday_cluster'] = -1\n",
    "train_df['weekend_cluster'] = -1\n",
    "\n",
    "for idx, num in enumerate(train_df['num'].unique()):\n",
    "    train_df.loc[train_df['num'] == num, 'weekday_cluster'] = weekday_clusters[idx]\n",
    "    train_df.loc[train_df['num'] == num, 'weekend_cluster'] = weekend_clusters[idx]\n",
    "\n",
    "# 각 클러스터 조합에 해당하는 건물 번호 출력\n",
    "cluster_combinations = {}\n",
    "weekday_labels = ['1', '2', '3']\n",
    "weekend_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "for w_label in weekday_labels:\n",
    "    for e_label in weekend_labels:\n",
    "        combination_label = (w_label, e_label)\n",
    "        cluster_combinations[combination_label] = []\n",
    "\n",
    "# 각 건물의 클러스터 레이블을 확인하여 조합에 추가\n",
    "unique_buildings = train_df['num'].unique()\n",
    "\n",
    "for num in unique_buildings:\n",
    "    weekday_cluster = weekday_clusters[num - 1]\n",
    "    weekend_cluster = weekend_clusters[num - 1]\n",
    "    \n",
    "    w_label = weekday_labels[weekday_cluster]\n",
    "    e_label = weekend_labels[weekend_cluster]\n",
    "    \n",
    "    combination_label = (w_label, e_label)\n",
    "    cluster_combinations[combination_label].append(num)\n",
    "\n",
    "# 각 클러스터 조합과 해당 건물 번호 출력\n",
    "for combination, buildings in cluster_combinations.items():\n",
    "    if buildings:\n",
    "        print(f\"클러스터 {combination}: 빌딩 {', '.join(map(str, buildings))}\")\n",
    "    else:\n",
    "        print(f\"클러스터 {combination}: 없음\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👌 프로펫 실행 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클러스터별로 변수를 생성하고, 해당되는 건물 번호만 모아서 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 클러스터 조합에 해당하는 빌딩 번호\n",
    "cluster_combinations = {\n",
    "    ('1', 'a'): [3, 5, 15, 24, 26, 28, 32, 33, 42, 44, 52, 57, 60],\n",
    "    ('1', 'b'): [],\n",
    "    ('1', 'c'): [18, 37, 46, 47, 55],\n",
    "    ('1', 'd'): [11, 40],\n",
    "    ('2', 'a'): [],\n",
    "    ('2', 'b'): [1, 8, 30, 31, 38, 54],\n",
    "    ('2', 'c'): [],\n",
    "    ('2', 'd'): [],\n",
    "    ('3', 'a'): [],\n",
    "    ('3', 'b'): [],\n",
    "    ('3', 'c'): [2, 4, 6, 7, 9, 13, 14, 16, 17, 19, 20, 21, 22, 23, 25, 27, 29, 34, 35, 36, 39, 41, 43, 45, 48, 49, 50, 51, 53, 56, 58, 59],\n",
    "    ('3', 'd'): [10, 12]\n",
    "}\n",
    "\n",
    "# 클러스터 조합별 데이터셋 저장할 딕셔너리\n",
    "clustered_dfs = {}\n",
    "\n",
    "# 각 클러스터 조합에 대해 데이터프레임 필터링 및 저장\n",
    "for combination, buildings in cluster_combinations.items():\n",
    "    if buildings:\n",
    "        clustered_dfs[combination] = train_df[train_df['num'].isin(buildings)]\n",
    "    else:\n",
    "        clustered_dfs[combination] = pd.DataFrame(columns=train_df.columns)\n",
    "\n",
    "# 클러스터별 데이터프레임을 변수에 할당\n",
    "for combination, df in clustered_dfs.items():\n",
    "    var_name = f\"cluster_{combination[0]}_{combination[1]}\"\n",
    "    globals()[var_name] = df\n",
    "\n",
    "# 확인을 위해 각 클러스터 조합별 데이터프레임의 크기 출력\n",
    "for combination in cluster_combinations.keys():\n",
    "    var_name = f\"cluster_{combination[0]}_{combination[1]}\"\n",
    "    print(f\"{var_name}: {len(globals()[var_name])} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과처럼 12개의 클러스터로 나누었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = cluster_1_a에 대하여 프로펫을 진행하면 이래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 😎 군집별로 각각 프로펫 돌려본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클러스터 데이터셋 딕셔너리\n",
    "clusters = {\n",
    "    'cluster_1_a': cluster_1_a,\n",
    "    # 'cluster_1_b': cluster_1_b,\n",
    "    'cluster_1_c': cluster_1_c,\n",
    "    'cluster_1_d': cluster_1_d,\n",
    "    # 'cluster_2_a': cluster_2_a,\n",
    "    'cluster_2_b': cluster_2_b,\n",
    "    # 'cluster_2_c': cluster_2_c,\n",
    "    # 'cluster_2_d': cluster_2_d,\n",
    "    # 'cluster_3_a': cluster_3_a,\n",
    "    # 'cluster_3_b': cluster_3_b,\n",
    "    'cluster_3_c': cluster_3_c,\n",
    "    'cluster_3_d': cluster_3_d\n",
    "}\n",
    "\n",
    "# 클러스터별로 Prophet 모델 학습 및 예측\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet 입력 형식에 맞게 데이터프레임 변환\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet 모델 생성 및 학습\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # 미래 데이터프레임 생성 (예측할 기간 설정, 예: 20일)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # 예측\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 예측 결과 시각화\n",
    "    fig1 = model.plot(forecast)\n",
    "    plt.title(f'{cluster_name} - Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    \n",
    "    # 검은색 점 크기 줄이기\n",
    "    for line in fig1.gca().get_lines():\n",
    "        if line.get_marker() == 'o':\n",
    "            line.set_markersize(1)  # 점의 크기를 1로 설정\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 😎 결과 해석\n",
    "\n",
    "예측 그래프 (`fig1`)\n",
    "\n",
    "1. **검은색 점 (실제 데이터 포인트)**:\n",
    "   - 이 점들은 실제 전력 소비 데이터를 나타냅니다. `cluster_1_a` 데이터셋의 `target` 값으로, 시간(`datetime`)에 따른 실제 전력 소비량을 보여줍니다.\n",
    "\n",
    "2. **파란색 선 (예측 값)**:\n",
    "   - 이 선은 Prophet 모델이 예측한 전력 소비량을 나타냅니다. 모델이 학습한 후 예측한 값으로, 실제 데이터와 비교하여 모델의 예측 성능을 평가할 수 있습니다.\n",
    "\n",
    "3. **파란색 음영 영역 (불확실성 범위)**:\n",
    "   - 이 영역은 예측의 불확실성 범위를 나타냅니다. Prophet 모델이 예측한 값의 신뢰 구간을 나타내며, 예측이 얼마나 정확한지를 나타냅니다.\n",
    "\n",
    "4. **날짜 축 (X축)**:\n",
    "   - X축은 날짜를 나타냅니다. 예측이 이루어진 시간 범위를 보여줍니다.\n",
    "\n",
    "5. **전력 소비량 축 (Y축)**:\n",
    "   - Y축은 전력 소비량을 나타냅니다. `target` 값으로, 예측된 전력 소비량의 단위를 나타냅니다.\n",
    "\n",
    "6. **제목 및 축 라벨**:\n",
    "   - 그래프의 제목은 \"Electricity Consumption Forecast\"로, 전력 소비 예측임을 나타냅니다. X축 라벨은 \"Date\", Y축 라벨은 \"Electricity Consumption (target)\"입니다.\n",
    "\n",
    "컴포넌트 그래프 (`fig2`)\n",
    "\n",
    "컴포넌트 그래프는 Prophet 모델의 예측 결과를 더 자세히 이해할 수 있도록 도와줍니다. 이 그래프는 예측에 기여하는 주요 요소들을 보여줍니다.\n",
    "\n",
    "1. **추세 (Trend)**:\n",
    "   - 전반적인 추세를 나타내며, 시간에 따른 전력 소비의 장기적인 변화 패턴을 보여줍니다.\n",
    "\n",
    "2. **주기성 (Yearly, Weekly, Daily Seasonality)**:\n",
    "   - 특정 주기(연간, 주간, 일간)마다 반복되는 패턴을 나타냅니다. 예를 들어, 전력 소비가 특정 시간대나 요일에 반복되는 패턴을 보여줍니다.\n",
    "\n",
    "3. **기타 요소 (Holidays, Special Events)**:\n",
    "   - 공휴일이나 특별한 이벤트와 같은 특정 이벤트에 따른 변화를 나타냅니다. 여기서는 특별히 추가된 이벤트가 없다면 표시되지 않을 수 있습니다.\n",
    "\n",
    "결론\n",
    "\n",
    "이 예측 결과를 통해 전력 소비 패턴을 이해하고, 미래의 전력 수요를 예측할 수 있습니다. 실제 데이터 포인트(검은색 점)와 모델 예측 값(파란색 선)을 비교하여 모델의 성능을 평가할 수 있으며, 불확실성 범위(파란색 음영)를 통해 예측의 신뢰도를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  😎 1~60번 건물을 전부 각각 프로펫 돌려본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유한 num 값을 추출\n",
    "unique_nums = train_df['num'].unique()\n",
    "\n",
    "# 각 num 값에 대해 데이터프레임을 필터링하고 변수에 할당\n",
    "for num in unique_nums:\n",
    "    globals()[f'num_{num}'] = train_df[train_df['num'] == num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클러스터 데이터셋 딕셔너리 생성\n",
    "clusters = {f'num_{i}': globals()[f'num_{i}'] for i in range(1, 61)}\n",
    "\n",
    "# 클러스터별로 Prophet 모델 학습 및 예측\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet 입력 형식에 맞게 데이터프레임 변환\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet 모델 생성 및 학습\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # 미래 데이터프레임 생성 (예측할 기간 설정, 예: 20일)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # 예측\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 예측 결과 시각화\n",
    "    fig1 = model.plot(forecast)\n",
    "    plt.title(f'{cluster_name} - Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    \n",
    "    # 검은색 점 크기 줄이기\n",
    "    for line in fig1.gca().get_lines():\n",
    "        if line.get_marker() == 'o':\n",
    "            line.set_markersize(1)  # 점의 크기를 1로 설정\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 Prophet 모델의 학습 결과를 RMSE값을 구하여 비교했다. RMSE값이 가장 낮은 건물은 32번 (18.9)이고, 가장 높은 건물은 8번(1945.2)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표를 저장할 리스트\n",
    "performance_metrics = []\n",
    "\n",
    "# 클러스터별로 Prophet 모델 학습 및 예측\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet 입력 형식에 맞게 데이터프레임 변환\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet 모델 생성 및 학습\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # 미래 데이터프레임 생성 (예측할 기간 설정, 예: 20일)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # 예측\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 성능 지표 계산 (예: MAE, RMSE)\n",
    "    y_true = df['y'].values\n",
    "    y_pred = forecast['yhat'][:len(y_true)].values  # 예측 결과 중 실제 데이터 길이만큼 사용\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    performance_metrics.append({\n",
    "        'Building': cluster_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "\n",
    "# 성능 지표를 데이터프레임으로 변환\n",
    "performance_df = pd.DataFrame(performance_metrics)\n",
    "\n",
    "# RMSE 값으로 데이터프레임 정렬\n",
    "performance_df = performance_df.sort_values(by='RMSE')\n",
    "\n",
    "# 성능 지표 출력\n",
    "print(performance_df)\n",
    "\n",
    "# 성능 지표 시각화 (RMSE만)\n",
    "performance_df.plot(x='Building', y='RMSE', kind='bar', figsize=(15, 7), legend=False)\n",
    "plt.title('Prediction RMSE by Building (Sorted)')\n",
    "plt.xlabel('Building')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👌 LSTM 돌려보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🐚 1~60번 건물 모두 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유한 num 값을 추출\n",
    "unique_nums = train_df['num'].unique()\n",
    "\n",
    "# 각 num 값에 대해 데이터프레임을 필터링하고 변수에 할당\n",
    "for num in unique_nums:\n",
    "    globals()[f'num_{num}'] = train_df[train_df['num'] == num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클러스터 데이터셋 로드\n",
    "df = num_1.copy()\n",
    "\n",
    "# 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "# LSTM 입력 형식에 맞게 데이터 준비\n",
    "sequence_length = 20\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "    y_train.append(scaled_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# LSTM 입력 형식에 맞게 데이터 재구성\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# LSTM 모델 생성\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# 미래 데이터를 예측하기 위해 준비\n",
    "test_data = scaled_data[-sequence_length:]\n",
    "x_test = [test_data]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# 예측\n",
    "predicted_values = []\n",
    "\n",
    "for _ in range(200):\n",
    "    prediction = model.predict(x_test)\n",
    "    predicted_values.append(prediction[0])\n",
    "    new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "    x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['target'], label='Actual')\n",
    "plt.plot(pd.date_range(start=df.index[-1], periods=201, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "plt.title('1A Electricity Consumption Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Consumption (target)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클러스터 데이터셋 로드\n",
    "df = num_60.copy()\n",
    "\n",
    "# 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "# LSTM 입력 형식에 맞게 데이터 준비\n",
    "sequence_length = 20\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "    y_train.append(scaled_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# LSTM 입력 형식에 맞게 데이터 재구성\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# LSTM 모델 생성\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# 미래 데이터를 예측하기 위해 준비\n",
    "test_data = scaled_data[-sequence_length:]\n",
    "x_test = [test_data]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# 예측\n",
    "predicted_values = []\n",
    "\n",
    "for _ in range(200):\n",
    "    prediction = model.predict(x_test)\n",
    "    predicted_values.append(prediction[0])\n",
    "    new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "    x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['target'], label='Actual')\n",
    "plt.plot(pd.date_range(start=df.index[-1], periods=201, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "plt.title('Electricity Consumption Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Consumption (target)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클러스터 데이터셋 로드\n",
    "clusters = {f'num_{i}': globals()[f'num_{i}'].copy() for i in range(1, 61)}\n",
    "\n",
    "# LSTM 모델 정의 함수\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 학습 및 예측 함수 정의\n",
    "def train_and_predict(df, sequence_length=20, epochs=50, batch_size=32, future_steps=200):\n",
    "    # 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # 데이터 스케일링\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM 입력 형식에 맞게 데이터 준비\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, len(scaled_data)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM 모델 생성 및 학습\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # 미래 데이터를 예측하기 위해 준비\n",
    "    test_data = scaled_data[-sequence_length:]\n",
    "    x_test = [test_data]\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # 예측\n",
    "    predicted_values = []\n",
    "    for _ in range(future_steps):\n",
    "        prediction = model.predict(x_test)\n",
    "        predicted_values.append(prediction[0])\n",
    "        new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "        x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    \n",
    "    # 예측 결과 시각화\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df.index, df['target'], label='Actual')\n",
    "    plt.plot(pd.date_range(start=df.index[-1], periods=future_steps + 1, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "    plt.title(f'{df.name} Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 모든 클러스터에 대해 모델 학습 및 예측\n",
    "for cluster_name, cluster_df in clusters.items():\n",
    "    cluster_df.name = cluster_name\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    train_and_predict(cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM 모델 정의 함수\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 학습 및 예측 함수 정의\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # 데이터 스케일링\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM 입력 형식에 맞게 데이터 준비\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_data) * 0.8)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM 모델 생성 및 학습\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_data = scaled_data[int(len(scaled_data) * 0.8) - sequence_length:]\n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data)):\n",
    "        x_test.append(test_data[i-sequence_length:i, 0])\n",
    "        y_test.append(test_data[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # 예측\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# 모든 클러스터에 대해 RMSE 계산\n",
    "rmses = {}\n",
    "for i in range(1, 61):  # 예제에서는 num_1부터 num_3까지 처리합니다. 실제로는 (1, 61)로 변경하세요.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    rmse = train_and_evaluate(df)\n",
    "    rmses[cluster_name] = rmse\n",
    "    print(f'Building {cluster_name} - RMSE: {rmse}')\n",
    "\n",
    "# RMSE 값 오름차순으로 정렬\n",
    "sorted_rmses = dict(sorted(rmses.items(), key=lambda item: item[1]))\n",
    "\n",
    "# RMSE 값을 데이터프레임으로 정리하여 출력\n",
    "rmse_df = pd.DataFrame(list(sorted_rmses.items()), columns=['Cluster', 'RMSE'])\n",
    "print(rmse_df)\n",
    "\n",
    "# RMSE 결과 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(rmse_df['Cluster'], rmse_df['RMSE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단변량으로 돌렸을 때\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM 모델 정의 함수\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# SMAPE 계산 함수 정의\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# 학습 및 예측 함수 정의\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # 데이터 스케일링\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM 입력 형식에 맞게 데이터 준비\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_data) * 0.8)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM 모델 생성 및 학습\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_data = scaled_data[int(len(scaled_data) * 0.8) - sequence_length:]\n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data)):\n",
    "        x_test.append(test_data[i-sequence_length:i, 0])\n",
    "        y_test.append(test_data[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # 예측\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # SMAPE 계산\n",
    "    smape_value = smape(y_test, predictions)\n",
    "    \n",
    "    return smape_value\n",
    "\n",
    "# 모든 클러스터에 대해 SMAPE 계산\n",
    "smapes = {}\n",
    "for i in range(1, 61):  # 예제에서는 num_1부터 num_60까지 처리합니다.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    smape_value = train_and_evaluate(df)\n",
    "    smapes[cluster_name] = smape_value\n",
    "    print(f'Building {cluster_name} - SMAPE: {smape_value}')\n",
    "\n",
    "# SMAPE 값 오름차순으로 정렬\n",
    "sorted_smapes = dict(sorted(smapes.items(), key=lambda item: item[1]))\n",
    "\n",
    "# SMAPE 값을 데이터프레임으로 정리하여 출력\n",
    "smape_df = pd.DataFrame(list(sorted_smapes.items()), columns=['Cluster', 'SMAPE'])\n",
    "print(smape_df)\n",
    "\n",
    "# SMAPE 결과 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(smape_df['Cluster'], smape_df['SMAPE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.title('SMAPE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변량으로 돌렸을 때\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM 모델 정의 함수\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# SMAPE 계산 함수 정의\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# 학습 및 예측 함수 정의\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # 데이터프레임을 'datetime'을 인덱스로 설정\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # 독립변수와 종속변수 분리\n",
    "    features = df.drop(columns=['target'])\n",
    "    target = df['target']\n",
    "\n",
    "    # 데이터 스케일링\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM 입력 형식에 맞게 데이터 준비\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_target) * 0.8)):\n",
    "        x_train.append(scaled_features[i-sequence_length:i, :])\n",
    "        y_train.append(scaled_target[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    # LSTM 모델 생성 및 학습\n",
    "    model = create_lstm_model((x_train.shape[1], x_train.shape[2]))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_data_features = scaled_features[int(len(scaled_target) * 0.8) - sequence_length:]\n",
    "    test_data_target = scaled_target[int(len(scaled_target) * 0.8) - sequence_length:]\n",
    "    \n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data_target)):\n",
    "        x_test.append(test_data_features[i-sequence_length:i, :])\n",
    "        y_test.append(test_data_target[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
    "\n",
    "    # 예측\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(np.column_stack([np.zeros((predictions.shape[0], scaled_features.shape[1]-1)), predictions]))[:, -1]\n",
    "    y_test = scaler.inverse_transform(np.column_stack([np.zeros((y_test.shape[0], scaled_features.shape[1]-1)), y_test.reshape(-1, 1)]))[:, -1]\n",
    "\n",
    "    # SMAPE 계산\n",
    "    smape_value = smape(y_test, predictions)\n",
    "    \n",
    "    return smape_value\n",
    "\n",
    "# 모든 클러스터에 대해 SMAPE 계산\n",
    "smapes = {}\n",
    "for i in range(1, 61):  # 예제에서는 num_1부터 num_60까지 처리합니다.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    smape_value = train_and_evaluate(df)\n",
    "    smapes[cluster_name] = smape_value\n",
    "    print(f'Building {cluster_name} - SMAPE: {smape_value}')\n",
    "\n",
    "# SMAPE 값 오름차순으로 정렬\n",
    "sorted_smapes = dict(sorted(smapes.items(), key=lambda item: item[1]))\n",
    "\n",
    "# SMAPE 값을 데이터프레임으로 정리하여 출력\n",
    "smape_df = pd.DataFrame(list(sorted_smapes.items()), columns=['Cluster', 'SMAPE'])\n",
    "print(smape_df)\n",
    "\n",
    "# SMAPE 결과 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(smape_df['Cluster'], smape_df['SMAPE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.title('SMAPE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  👌클러스터별로 아리마 돌려본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_1_a['datetime'] = pd.to_datetime(cluster_1_a['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_1_a['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_a[cluster_1_a['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_1_c['datetime'] = pd.to_datetime(cluster_1_c['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_1_c['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_c[cluster_1_c['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_1_d['datetime'] = pd.to_datetime(cluster_1_d['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_1_d['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_d[cluster_1_d['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_2_b['datetime'] = pd.to_datetime(cluster_2_b['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_2_b['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_2_b[cluster_2_b['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_3_c['datetime'] = pd.to_datetime(cluster_3_c['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_3_c['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_3_c[cluster_3_c['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' 컬럼을 datetime 형식으로 변환\n",
    "cluster_3_d['datetime'] = pd.to_datetime(cluster_3_d['datetime'])\n",
    "\n",
    "# unique 'num' 값들\n",
    "unique_nums = cluster_3_d['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_3_d[cluster_3_d['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터에서 돌리기 어려우니 코랩에서 돌리는 것을 권장!\n",
    "# 하단 주석 처리는 Pycaret을 위한 것입니다. Pycaret 사용 시 주석 해제 필요!\n",
    "\n",
    "!pip install pycaret\n",
    "import os\n",
    "\n",
    "# 오직 중요한 로깅만 활성화하기 (선택적)\n",
    "os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "\n",
    "# 설치된 라이브러리 확인\n",
    "def what_is_installed():\n",
    "    from pycaret import show_versions\n",
    "    show_versions()\n",
    "\n",
    "try:\n",
    "    what_is_installed()\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pycaret\n",
    "    what_is_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "!pip install sktime\n",
    "\n",
    "# data analysis and wrangling\n",
    "import sys\n",
    "import sktime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import scipy.stats\n",
    "import tqdm as tq\n",
    "# import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import missingno as msno\n",
    "\n",
    "# EDA & visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm           # 회귀 / 시계열 ... 일반적인 모델\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor # 다중공선성\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Time series\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "# ML Algorithms\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# model validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "\n",
    "# deep-learning libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, GRU, RNN\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")\n",
    "print(f\"joblib version: {joblib.__version__}\")\n",
    "print(f\"missingno version: {msno.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"sktime version: {sktime.__version__}\")\n",
    "print(f\"scikit-learn version: {skl.__version__}\")\n",
    "# print(f\"lightgbm version: {lgb.__version__}\")\n",
    "# print(f\"catboost version: {catboost.__version__}\")\n",
    "print(f\"tqdm version: {tq.__version__}\")\n",
    "# print(f\"xgboost version: {xgb.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "print(\"------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\train.csv', encoding='cp949')\n",
    "test_df = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\test.csv', encoding='cp949')\n",
    "sub = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\sample_submission.csv', encoding='cp949')\n",
    "\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "\n",
    "# 데이터 형태 및 개수, 결측치 확인\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100\n",
    "\n",
    "def datetimeTransform(data):\n",
    "    data.insert(0, \"date_time\", pd.to_datetime(data[\"datetime\"]))\n",
    "    data.drop(columns=[\"datetime\"], inplace=True)\n",
    "    return data.head(25)\n",
    "\n",
    "df_dict = {}\n",
    "def separateDataFrame(data):\n",
    "    for i in range(1, 61):\n",
    "        df_dict[i] = data[data['num'] == i]\n",
    "    return df_dict\n",
    "\n",
    "def CDH(xs):\n",
    "    ys = []\n",
    "    for i in range(len(xs)):\n",
    "        if i < 11:\n",
    "            ys.append(np.sum(xs[:(i+1)]-26))\n",
    "        else:\n",
    "            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "    return np.array(ys)\n",
    "\n",
    "def THI(data) :\n",
    "    data['THI'] = round(1.8*data['temperature'] - 0.55*(1-(data['humidity']/100))*(1.8*data['temperature']-26) + 32, 2)\n",
    "    # print(data['THI'].head())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df -> 건물번호 별 데이터프레임 정렬\n",
    "\n",
    "# 냉방도시간 CDH(Cooling Degree Hour)\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = train_df[train_df['num'] == num]\n",
    "    cdh = CDH(temp['temperature'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "train_df['CDH'] = cdhs\n",
    "\n",
    "# 불쾌지수 함수\n",
    "THI(train_df)\n",
    "# 판다스 datetime 변환\n",
    "datetimeTransform(train_df)\n",
    "# 비전기냉방, 태양광 변수 삭제\n",
    "# 60개 각각의 동일 건물별 모델링 시 비전기냉방과 태양광 여부는 필요없음 -> 삭제\n",
    "train_df.drop(columns=['nelec_cool_flag','solar_flag'], inplace=True)\n",
    "# 건물번호 구분 및 데이터 저장 함수\n",
    "separateDataFrame(train_df)\n",
    "df_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 처리 일조\n",
    "# discrete한 분포로 가장 최근에 기록된 일조값을 가져오는 형태로 결측치 보간\n",
    "train_df['insolation'].value_counts()\n",
    "test_df['insolation'] = test_df['insolation'].interpolate(method='pad')\n",
    "\n",
    "# 결측치 채우기: 기온, 풍속, 습도, 강수량\n",
    "# 기온, 풍속, 습도, 강수량은 'pad','linear','quadratic','cubic' 방법 중\n",
    "# train 데이터에서 가장 성능이 뛰어난 보간법으로 결측치 보간 시행\n",
    "# 우선 train 데이터 임의로 결측치를 생성\n",
    "train_ = train_df.copy(deep=True)\n",
    "def make_train_nan(col, n):\n",
    "    new_list = []\n",
    "    for idx, temp in enumerate(train_[col]):\n",
    "        if idx%n==0:\n",
    "            new_list.append(temp)\n",
    "        else:\n",
    "            new_list.append(np.nan)\n",
    "    train_['{}'.format(col+'_nan')] = new_list\n",
    "make_train_nan('temperature',3)\n",
    "make_train_nan('windspeed',3)\n",
    "make_train_nan('humidity',3)\n",
    "make_train_nan('precipitation',6)\n",
    "print(train_.iloc[:,-4:].isnull().sum())\n",
    "\n",
    "# 결측치가 잘 생성됨을 확인하였으며 각 변수에 대해 4가지의 보간법을 시행한 후\n",
    "# 가장 오차가 적은 보간법으로 결측치 보간\n",
    "def compare_interpolate_methods(col, methods, metric):\n",
    "    error_dict = dict()\n",
    "    for method in methods:\n",
    "        fillna = train_['{}'.format(col+'_nan')].interpolate(method=method)\n",
    "        if fillna.isna().sum() != 0:\n",
    "            fillna = fillna.interpolate(method='linear')\n",
    "        error = metric(train_['{}'.format(col)], fillna)\n",
    "        error_dict['{}'.format(method)] = error\n",
    "\n",
    "    return error_dict\n",
    "all_cols_error_dict = dict()\n",
    "for col in ['temperature', 'windspeed', 'humidity', 'precipitation']:\n",
    "    methods = ['pad','linear','quadratic','cubic']\n",
    "    error_dict = compare_interpolate_methods(col, methods, mean_squared_error)\n",
    "    all_cols_error_dict['{}'.format(col)] = error_dict\n",
    "\n",
    "all_cols_error_df = pd.DataFrame(all_cols_error_dict)\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize = (18,5), sharey=False)\n",
    "for i in range(len(all_cols_error_df.columns)):\n",
    "    sns.lineplot(ax=axes[i], data=all_cols_error_df.iloc[:,i].transpose())\n",
    "\n",
    "#기온 결측치 채우기\n",
    "test_df['temperature'] = test_df['temperature'].interpolate(method='quadratic')\n",
    "#마지막 na 채우기\n",
    "test_df['temperature'] = test_df['temperature'].interpolate(method='linear')\n",
    "\n",
    "#풍속 결측치 채우기\n",
    "test_df['windspeed'] = test_df['windspeed'].interpolate(method='linear')\n",
    "\n",
    "#습도 결측치 채우기\n",
    "test_df['humidity'] = test_df['humidity'].interpolate(method='quadratic')\n",
    "#마지막 na 채우기\n",
    "test_df['humidity'] = test_df['humidity'].interpolate(method='linear')\n",
    "\n",
    "#강수량 결측치 채우기\n",
    "test_df['precipitation'] = test_df['precipitation'].interpolate(method='linear')\n",
    "\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df -> 건물번호 별 데이터프레임 정렬\n",
    "\n",
    "# # 결측값 처리 pandas 내 선형보간 method 사용\n",
    "# for i in range(60):\n",
    "#     test_df.iloc[i*168:(i+1)*168, :]  = test_df.iloc[i*168:(i+1)*168, :].interpolate()\n",
    "\n",
    "# 냉방도시간 CDH(Cooling Degree Hour)\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = test_df[test_df['num'] == num]\n",
    "    cdh = CDH(temp['temperature'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "test_df['CDH'] = cdhs\n",
    "\n",
    "# 불쾌지수 함수\n",
    "THI(test_df)\n",
    "# 판다스 datetime 변환\n",
    "datetimeTransform(test_df)\n",
    "# 비전기냉방, 태양광 변수 삭제\n",
    "# 60개 각각의 동일 건물별 모델링 시 비전기냉방과 태양광 여부는 필요없음 -> 삭제\n",
    "test_df.drop(columns=['nelec_cool_flag','solar_flag'], inplace=True)\n",
    "# 건물번호 구분 및 데이터 저장 함수\n",
    "test_df_dict = {}\n",
    "for i in range(1, 61):\n",
    "    test_df_dict[i] = test_df[test_df['num'] == i]\n",
    "test_df_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 건물 별 train_df test_df 의 건물번호를 삭제함\n",
    "for i in range(1,61):\n",
    "    df_dict[i].reset_index(drop=True, inplace=True)\n",
    "    df_dict[i].drop(columns=['num'], inplace=True)\n",
    "    test_df_dict[i].reset_index(drop=True, inplace=True)\n",
    "    test_df_dict[i].drop(columns=['num'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numPlot(data,i:int):\n",
    "    ''' 건물번호로 분류한 시간에 따른 전력사용량 함수'''\n",
    "    num_i = data[i].copy(deep=True)\n",
    "\n",
    "    num_i.set_index('date_time',inplace=True)\n",
    "    num_i.index = pd.to_datetime(num_i.index)\n",
    "    return num_i.loc[:,'target']\n",
    "numPlot(df_dict,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pycaret.anomaly import *\n",
    "\n",
    "# Assuming df_dict is already defined and contains your data\n",
    "df_dict1 = {}\n",
    "anomaly_counts = {}\n",
    "interpolated_counts = {}\n",
    "\n",
    "# Create subplots for anomaly detection results\n",
    "fig_anomaly = make_subplots(rows=15, cols=4, subplot_titles=[f'Building {i}' for i in range(1, 61)])\n",
    "# Create subplots for interpolated data results\n",
    "fig_interpolated = make_subplots(rows=15, cols=4, subplot_titles=[f'Building {i}' for i in range(1, 61)])\n",
    "\n",
    "for i in range(1, 61):\n",
    "    data = df_dict[i].copy(deep=True)\n",
    "\n",
    "    # set timestamp to index\n",
    "    data.set_index('date_time', drop=True, inplace=True)\n",
    "\n",
    "    # resample timeseries to hourly\n",
    "    data = data.resample('H').sum()\n",
    "\n",
    "    # create features from date\n",
    "    data['day'] = [i.day for i in data.index]\n",
    "    data['day_name'] = [i.day_name() for i in data.index]\n",
    "    data['day_of_year'] = [i.dayofyear for i in data.index]\n",
    "    data['week_of_year'] = [i.isocalendar()[1] for i in data.index]\n",
    "    data['hour'] = [i.hour for i in data.index]\n",
    "    data['is_weekday'] = [i.isoweekday() for i in data.index]\n",
    "\n",
    "    # init setup\n",
    "    s = setup(data, session_id=42,\n",
    "              ordinal_features={'day_name': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday', 'Saturday']},\n",
    "              numeric_features=['is_weekday'])\n",
    "\n",
    "    # train model\n",
    "    lof = create_model('lof')\n",
    "    lof_results = assign_model(lof)\n",
    "\n",
    "    # count anomalies\n",
    "    anomaly_count = lof_results['Anomaly'].sum()\n",
    "    anomaly_counts[i] = anomaly_count\n",
    "\n",
    "    # filter out anomalies\n",
    "    filtered_data = lof_results[lof_results['Anomaly'] != 1]\n",
    "\n",
    "    # interpolate missing values\n",
    "    interpolated_data = filtered_data.copy()\n",
    "    interpolated_data['target'] = interpolated_data['target'].interpolate()\n",
    "\n",
    "    # count interpolated values\n",
    "    interpolated_count = interpolated_data['target'].isna().sum()\n",
    "    interpolated_counts[i] = interpolated_count\n",
    "\n",
    "    # Add anomaly detection plot to subplot\n",
    "    outlier_dates = lof_results[lof_results['Anomaly'] == 1].index\n",
    "    y_values = [lof_results.loc[j]['target'] for j in outlier_dates]\n",
    "    row = (i-1) // 4 + 1\n",
    "    col = (i-1) % 4 + 1\n",
    "    fig_anomaly.add_trace(go.Scatter(x=lof_results.index, y=lof_results['target'], mode='lines', name=f'Building {i}'),\n",
    "                          row=row, col=col)\n",
    "    fig_anomaly.add_trace(go.Scatter(x=outlier_dates, y=y_values, mode='markers', name='Anomaly', marker=dict(color='red', size=5)),\n",
    "                          row=row, col=col)\n",
    "\n",
    "    # Add interpolated data plot to subplot\n",
    "    fig_interpolated.add_trace(go.Scatter(x=interpolated_data.index, y=interpolated_data['target'], mode='lines', name=f'Building {i}'),\n",
    "                               row=row, col=col)\n",
    "\n",
    "    df_dict1[i] = lof_results\n",
    "    df_dict1[i].drop(columns=['day', 'day_name', 'day_of_year', 'week_of_year', 'hour', 'is_weekday', 'Anomaly', 'Anomaly_Score'], inplace=True)\n",
    "    df_dict1[i].reset_index(inplace=True)\n",
    "    print(df_dict1[i])\n",
    "\n",
    "# Update layout and show anomaly detection subplot\n",
    "fig_anomaly.update_layout(height=3000, width=1200, title_text='전력사용량 - UNSUPERVISED ANOMALY DETECTION', showlegend=False)\n",
    "fig_anomaly.show()\n",
    "\n",
    "# Update layout and show interpolated data subplot\n",
    "fig_interpolated.update_layout(height=3000, width=1200, title_text='전력사용량 - Interpolated Data (Outliers Removed)', showlegend=False)\n",
    "fig_interpolated.show()\n",
    "\n",
    "# Print anomaly and interpolation counts\n",
    "for i in range(1, 61):\n",
    "    print(f\"Building {i}: {anomaly_counts[i]} anomalies, {interpolated_counts[i]} interpolations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4조_분석코드(2차)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
