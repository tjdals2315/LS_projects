{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì „ì²˜ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train.csv', encoding='cp949')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## ë³€ìˆ˜ë“¤ì„ ì˜ë¬¸ëª…ìœ¼ë¡œ ë³€ê²½\n",
    "cols = ['num', 'date_time', 'power', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "train.columns = cols\n",
    "\n",
    "# ì‹œê°„ ê´€ë ¨ ë³€ìˆ˜ë“¤ ìƒì„±\n",
    "date = pd.to_datetime(train.date_time)\n",
    "train['hour'] = date.dt.hour\n",
    "train['day'] = date.dt.weekday\n",
    "train['month'] = date.dt.month\n",
    "train['week'] = date.dt.weekofyear\n",
    "\n",
    "\n",
    "## ê±´ë¬¼ë³„, ìš”ì¼ë³„, ì‹œê°„ë³„ ë°œì „ëŸ‰ í‰ê·  ë„£ì–´ì£¼ê¸°\n",
    "\n",
    "power_mean = pd.pivot_table(train, values = 'power', index = ['num', 'hour', 'day'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['day_hour_mean'] = train.progress_apply(lambda x : power_mean.loc[(power_mean.num == x['num']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "\n",
    "## ê±´ë¬¼ë³„ ì‹œê°„ë³„ ë°œì „ëŸ‰ í‰ê·  ë„£ì–´ì£¼ê¸°\n",
    " \n",
    "power_hour_mean = pd.pivot_table(train, values = 'power', index = ['num', 'hour'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_mean'] = train.progress_apply(lambda x : power_hour_mean.loc[(power_hour_mean.num == x['num']) & (power_hour_mean.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "\n",
    "## ê±´ë¬¼ë³„ ì‹œê°„ë³„ ë°œì „ëŸ‰ í‘œì¤€í¸ì°¨ ë„£ì–´ì£¼ê¸°\n",
    "\n",
    "power_hour_std = pd.pivot_table(train, values = 'power', index = ['num', 'hour'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_std'] = train.progress_apply(lambda x : power_hour_std.loc[(power_hour_std.num == x['num']) & (power_hour_std.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "### ê³µíœ´ì¼ ë³€ìˆ˜ ì¶”ê°€\n",
    "train['holiday'] = train.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n",
    "train.loc[('2020-08-17'<=train.date_time)&(train.date_time<'2020-08-18'), 'holiday'] = 1\n",
    "\n",
    "## https://dacon.io/competitions/official/235680/codeshare/2366?page=1&dtype=recent\n",
    "train['sin_time'] = np.sin(2*np.pi*train.hour/24)\n",
    "train['cos_time'] = np.cos(2*np.pi*train.hour/24)\n",
    "\n",
    "## https://dacon.io/competitions/official/235736/codeshare/2743?page=1&dtype=recent\n",
    "train['THI'] = 9/5*train['temp'] - 0.55*(1-train['hum']/100)*(9/5*train['hum']-26)+32\n",
    "\n",
    "def CDH(xs):\n",
    "    ys = []\n",
    "    for i in range(len(xs)):\n",
    "        if i < 11:\n",
    "            ys.append(np.sum(xs[:(i+1)]-26))\n",
    "        else:\n",
    "            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "    return np.array(ys)\n",
    "\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = train[train['num'] == num]\n",
    "    cdh = CDH(temp['temp'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "train['CDH'] = cdhs\n",
    "\n",
    "train.drop(['non_elec','solar','hour'], axis = 1, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the preprocessed data\n",
    "train.to_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA/SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CSV íŒŒì¼ì—ì„œ ì „ì²´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv', parse_dates=['date_time'], index_col='date_time')\n",
    "\n",
    "# ê±´ë¬¼ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "building_nums = df['num'].unique()\n",
    "\n",
    "# ê° ê±´ë¬¼ì— ëŒ€í•´ ARIMA ëª¨ë¸ ì ìš© ë° ì˜ˆì¸¡ ì‹œê°í™”\n",
    "for num in building_nums:\n",
    "    # ê±´ë¬¼ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "    building_data = df[df['num'] == num]['power']\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    train_size = int(len(building_data) * 0.8)\n",
    "    train, test = building_data.iloc[:train_size], building_data.iloc[train_size:]\n",
    "    \n",
    "    # ARIMA ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "    model = ARIMA(train, order=(1, 1, 1))\n",
    "    arima_fit = model.fit()\n",
    "    pred = arima_fit.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "    \n",
    "    # í‰ê°€ ë° ì‹œê°í™”\n",
    "    mse = mean_squared_error(test, pred)\n",
    "    print(f\"Building {num} - Mean Squared Error: {mse}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Test')\n",
    "    plt.plot(test.index, pred, label='Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Building {num} - ARIMA Model Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# CSV íŒŒì¼ì—ì„œ ì „ì²´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(r'C:\\Users\\USER\\projects\\0527p\\train_preprocessed.csv', parse_dates=['date_time'], index_col='date_time')\n",
    "\n",
    "# ê±´ë¬¼ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "building_nums = df['num'].unique()\n",
    "\n",
    "# ê° ê±´ë¬¼ì— ëŒ€í•´ SARIMA ëª¨ë¸ ì ìš© ë° ì˜ˆì¸¡ ì‹œê°í™”\n",
    "for num in building_nums:\n",
    "    # ê±´ë¬¼ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "    building_data = df[df['num'] == num]['power']\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    train_size = int(len(building_data) * 0.8)\n",
    "    train, test = building_data.iloc[:train_size], building_data.iloc[train_size:]\n",
    "    \n",
    "    # SARIMA ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "    model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
    "    sarima_fit = model.fit(disp=False)\n",
    "    pred = sarima_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "    \n",
    "    # í‰ê°€ ë° ì‹œê°í™”\n",
    "    mse = mean_squared_error(test, pred)\n",
    "    print(f\"Building {num} - Mean Squared Error: {mse}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Test')\n",
    "    plt.plot(test.index, pred, label='Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Building {num} - SARIMA Model Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF/PADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv',encoding='euc-kr', parse_dates=['date_time'])\n",
    "test_df = pd.read_csv('test.csv', encoding='euc-kr', parse_dates=['date_time'])\n",
    "sub = pd.read_csv('sample_submission.csv', encoding='euc-kr')\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "data=train_df\n",
    "\n",
    "# 'datetime' ì—´ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "# 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['target'], label='Energy Usage (kWh)')\n",
    "plt.title('Energy Usage Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Energy Usage (kWh)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ADF í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "def adf_test(series):\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "\n",
    "# 'target' ì—´ì— ëŒ€í•´ ADF í…ŒìŠ¤íŠ¸ ìˆ˜í–‰\n",
    "adf_test(data['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì • ë¹Œë”© ë°ì´í„° ì„ íƒ\n",
    "building_num = 1\n",
    "df = data[data['num'] == building_num]\n",
    "\n",
    "# ACF ë° PACF í”Œë¡¯ ìƒì„±\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(df['target'], ax=plt.gca())\n",
    "plt.title(f'ACF for Building {building_num}')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(df['target'], method='ywm', ax=plt.gca())\n",
    "plt.title(f'PACF for Building {building_num}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# ADF í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "def adf_test(series):\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "\n",
    "# 'target' ì—´ì— ëŒ€í•´ ADF í…ŒìŠ¤íŠ¸ ìˆ˜í–‰\n",
    "adf_test(data['target'])\n",
    "\n",
    "# ACF ë° PACF í”Œë¡¯ ìƒì„±\n",
    "num_buildings = 60\n",
    "fig, axes = plt.subplots(num_buildings, 2, figsize=(15, 3*num_buildings))\n",
    "\n",
    "for num in range(1, num_buildings + 1):\n",
    "    df = train_df[train_df.num == num]\n",
    "    \n",
    "    # ACF plot\n",
    "    plot_acf(df['target'], ax=axes[num-1, 0])\n",
    "    axes[num-1, 0].set_title(f'ACF for Building {num}')\n",
    "    \n",
    "    # PACF plot\n",
    "    plot_pacf(df['target'], method='ywm', ax=axes[num-1, 1])\n",
    "    axes[num-1, 1].set_title(f'PACF for Building {num}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ğŸ‘Œêµ°ì§‘í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\train.csv', encoding='euc|-kr', parse_dates=['date_time'])\n",
    "test_df = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\electricity_data\\test.csv', encoding='euc-kr', parse_dates=['date_time'])\n",
    "sub = pd.read_csv(r'C:\\Users\\USER\\projects\\0521p\\sample_submission.csv', encoding='euc-kr')\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "\n",
    "# ë°ì´í„° í˜•íƒœ ë° ê°œìˆ˜, ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ë‚ ì§œí˜•ì‹ìœ¼ë¡œ ë°”ê¿ˆ\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "\n",
    "# ì‹œê°„ ì—´ ì¶”ê°€\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "\n",
    "hourly_mean = pd.DataFrame()\n",
    "\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "train_df['weekend'] = train_df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# ì£¼ì¤‘ ë°ì´í„° í•„í„°ë§ (ì£¼ì¤‘ì€ weekdayê°€ 0ë¶€í„° 4ê¹Œì§€ì¸ ë‚ ì§œ)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'datetime'ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì‹œê°„ ì¶”ì¶œ\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "\n",
    "# ì£¼ì¤‘ ë°ì´í„° í•„í„°ë§ (ì£¼ì¤‘ì€ weekdayê°€ 0ë¶€í„° 4ê¹Œì§€ì¸ ë‚ ì§œ)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n",
    "\n",
    "# ê° ê±´ë¬¼ ë³„, ì‹œê°„ëŒ€ ë³„ í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ê³„ì‚°\n",
    "hourly_consumption = weekday_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "\n",
    "# í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜\n",
    "feature_matrix = hourly_consumption.fillna(0).values\n",
    "\n",
    "# ì—˜ë³´ìš° ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°\n",
    "sse = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)  # 2ë¶€í„° 10ê¹Œì§€ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì‹œë„\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(feature_matrix, kmeans.labels_))\n",
    "\n",
    "# # ì—˜ë³´ìš° ë°©ë²• ê·¸ë˜í”„\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_range, sse, marker='o')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Sum of squared distances (SSE)')\n",
    "# plt.show()\n",
    "\n",
    "# # ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê·¸ë˜í”„\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_range, silhouette_scores, marker='o')\n",
    "# plt.title('Silhouette Scores For Optimal k')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.show()\n",
    "\n",
    "# KMeans êµ°ì§‘í™” ìˆ˜í–‰ (ìµœì ì˜ k ì„ íƒ í›„, ì˜ˆ: 6)\n",
    "optimal_k = 6  # ì—˜ë³´ìš° ë°©ë²•ê³¼ ì‹¤ë£¨ì—£ ê³„ìˆ˜ë¥¼ í†µí•´ ê²°ì •ëœ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "clusters = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "clustered_buildings = {}\n",
    "for building_idx, cluster_label in enumerate(clusters):\n",
    "    if cluster_label not in clustered_buildings:\n",
    "        clustered_buildings[cluster_label] = []\n",
    "    clustered_buildings[cluster_label].append(building_idx + 1)  \n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥\n",
    "for cluster, buildings in clustered_buildings.items():\n",
    "    print(f\"í´ëŸ¬ìŠ¤í„° {cluster + 1}: ë¹Œë”© {', '.join(map(str, buildings))}\")\n",
    "\n",
    "# # ì‹œê°í™”: í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ íŒ¨í„´\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# for cluster_num in range(optimal_k):\n",
    "#     cluster_data = hourly_consumption.iloc[np.where(clusters == cluster_num)].mean(axis=0)\n",
    "    \n",
    "#     plt.plot(cluster_data.index, cluster_data.values, label=f'Cluster {cluster_num + 1}')\n",
    "\n",
    "# plt.title('Mean Hourly Energy Consumption by Cluster (Weekdays)')\n",
    "# plt.xlabel('Hour of Day')\n",
    "# plt.ylabel('Mean Energy Consumption')\n",
    "# plt.legend(title='Cluster')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜ ë° ì‹œê°„ ì¶”ì¶œ\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "train_df['weekday'] = train_df['datetime'].dt.dayofweek\n",
    "\n",
    "# ì£¼ì¤‘ ë°ì´í„° í•„í„°ë§ (ì£¼ì¤‘ì€ weekdayê°€ 0ë¶€í„° 4ê¹Œì§€ì¸ ë‚ ì§œ)\n",
    "weekday_data = train_df[train_df['weekday'].isin([0, 1, 2, 3, 4])]\n",
    "# ì£¼ë§ ë°ì´í„° í•„í„°ë§ (ì£¼ë§ì€ weekdayê°€ 5ë¶€í„° 6ê¹Œì§€ì¸ ë‚ ì§œ)\n",
    "weekend_data = train_df[train_df['weekday'].isin([5, 6])]\n",
    "\n",
    "# ê° ê±´ë¬¼ ë³„, ì‹œê°„ëŒ€ ë³„ í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ê³„ì‚° (ì£¼ì¤‘)\n",
    "weekday_hourly_consumption = weekday_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "# ê° ê±´ë¬¼ ë³„, ì‹œê°„ëŒ€ ë³„ í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ê³„ì‚° (ì£¼ë§)\n",
    "weekend_hourly_consumption = weekend_data.groupby(['num', 'hour'])['target'].mean().unstack()\n",
    "\n",
    "# í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ (ì£¼ì¤‘, ì£¼ë§)\n",
    "weekday_feature_matrix = weekday_hourly_consumption.fillna(0).values\n",
    "weekend_feature_matrix = weekend_hourly_consumption.fillna(0).values\n",
    "\n",
    "# KMeans êµ°ì§‘í™” ìˆ˜í–‰ (ì£¼ì¤‘: 3 í´ëŸ¬ìŠ¤í„°, ì£¼ë§: 4 í´ëŸ¬ìŠ¤í„°)\n",
    "optimal_k_weekday = 3\n",
    "optimal_k_weekend = 4\n",
    "\n",
    "kmeans_weekday = KMeans(n_clusters=optimal_k_weekday, random_state=0)\n",
    "weekday_clusters = kmeans_weekday.fit_predict(weekday_feature_matrix)\n",
    "\n",
    "kmeans_weekend = KMeans(n_clusters=optimal_k_weekend, random_state=0)\n",
    "weekend_clusters = kmeans_weekend.fit_predict(weekend_feature_matrix)\n",
    "\n",
    "def print_clusters(clusters, labels, title):\n",
    "    clustered_buildings = {}\n",
    "    for building_idx, cluster_label in enumerate(clusters):\n",
    "        if labels[cluster_label] not in clustered_buildings:\n",
    "            clustered_buildings[labels[cluster_label]] = []\n",
    "        clustered_buildings[labels[cluster_label]].append(building_idx + 1)\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    for cluster, buildings in clustered_buildings.items():\n",
    "        print(f\"í´ëŸ¬ìŠ¤í„° {cluster}: ë¹Œë”© {', '.join(map(str, buildings))}\")\n",
    "\n",
    "# ì£¼ì¤‘ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥ (1, 2, 3)\n",
    "weekday_labels = ['1', '2', '3']\n",
    "print_clusters(weekday_clusters, weekday_labels, \"Weekday Clusters\")\n",
    "\n",
    "# ì£¼ë§ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥ (a, b, c, d)\n",
    "weekend_labels = ['a', 'b', 'c', 'd']\n",
    "print_clusters(weekend_clusters, weekend_labels, \"Weekend Clusters\")\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° êµì°¨í‘œ ì‘ì„±\n",
    "cross_tab = pd.crosstab(\n",
    "    pd.Series(weekday_clusters, name='Weekday Cluster').map({0: '1', 1: '2', 2: '3'}),\n",
    "    pd.Series(weekend_clusters, name='Weekend Cluster').map({0: 'a', 1: 'b', 2: 'c', 3: 'd'})\n",
    ")\n",
    "\n",
    "# ë³´ê¸° ì¢‹ê²Œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì¶œë ¥\n",
    "cross_tab_df = cross_tab.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì ìš©í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "def style_crosstab(df):\n",
    "    styled_df = df.style.set_table_styles(\n",
    "        [{'selector': 'thead th', 'props': [('background-color', '#f7f7f9'), ('color', 'black')]}]\n",
    "    ).set_properties(**{\n",
    "        'background-color': '#e8e8e8', \n",
    "        'color': 'black', \n",
    "        'border-color': 'white',\n",
    "        'text-align': 'center'\n",
    "    }).set_caption('Cross Tabulation of Weekday and Weekend Clusters').format(na_rep='').hide(axis=\"index\")\n",
    "    return styled_df\n",
    "\n",
    "styled_crosstab = style_crosstab(cross_tab_df)\n",
    "display(styled_crosstab)\n",
    "\n",
    "# # ì‹œê°í™”: í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ì—ë„ˆì§€ ì†Œë¹„ íŒ¨í„´ (ì£¼ì¤‘, ì£¼ë§)\n",
    "# def plot_cluster_patterns(feature_matrix, clusters, optimal_k, labels, title):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     for cluster_num in range(optimal_k):\n",
    "#         cluster_data = pd.DataFrame(feature_matrix).iloc[np.where(clusters == cluster_num)].mean(axis=0)\n",
    "#         plt.plot(cluster_data.index, cluster_data.values, label=f'Cluster {labels[cluster_num]}')\n",
    "#     plt.title(f'Mean Hourly Energy Consumption by Cluster ({title})')\n",
    "#     plt.xlabel('Hour of Day')\n",
    "#     plt.ylabel('Mean Energy Consumption')\n",
    "#     plt.legend(title='Cluster')\n",
    "#     # plt.show()\n",
    "\n",
    "# plot_cluster_patterns(weekday_feature_matrix, weekday_clusters, optimal_k_weekday, weekday_labels, \"Weekday\")\n",
    "# plot_cluster_patterns(weekend_feature_matrix, weekend_clusters, optimal_k_weekend, weekend_labels, \"Weekend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ì£¼ì¤‘ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ê³¼ ì£¼ë§ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ ê°ê°ì˜ ì—´ë¡œ ì¶”ê°€\n",
    "train_df['weekday_cluster'] = -1\n",
    "train_df['weekend_cluster'] = -1\n",
    "\n",
    "for idx, num in enumerate(train_df['num'].unique()):\n",
    "    train_df.loc[train_df['num'] == num, 'weekday_cluster'] = weekday_clusters[idx]\n",
    "    train_df.loc[train_df['num'] == num, 'weekend_cluster'] = weekend_clusters[idx]\n",
    "\n",
    "# ê° í´ëŸ¬ìŠ¤í„° ì¡°í•©ì— í•´ë‹¹í•˜ëŠ” ê±´ë¬¼ ë²ˆí˜¸ ì¶œë ¥\n",
    "cluster_combinations = {}\n",
    "weekday_labels = ['1', '2', '3']\n",
    "weekend_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "for w_label in weekday_labels:\n",
    "    for e_label in weekend_labels:\n",
    "        combination_label = (w_label, e_label)\n",
    "        cluster_combinations[combination_label] = []\n",
    "\n",
    "# ê° ê±´ë¬¼ì˜ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ í™•ì¸í•˜ì—¬ ì¡°í•©ì— ì¶”ê°€\n",
    "unique_buildings = train_df['num'].unique()\n",
    "\n",
    "for num in unique_buildings:\n",
    "    weekday_cluster = weekday_clusters[num - 1]\n",
    "    weekend_cluster = weekend_clusters[num - 1]\n",
    "    \n",
    "    w_label = weekday_labels[weekday_cluster]\n",
    "    e_label = weekend_labels[weekend_cluster]\n",
    "    \n",
    "    combination_label = (w_label, e_label)\n",
    "    cluster_combinations[combination_label].append(num)\n",
    "\n",
    "# ê° í´ëŸ¬ìŠ¤í„° ì¡°í•©ê³¼ í•´ë‹¹ ê±´ë¬¼ ë²ˆí˜¸ ì¶œë ¥\n",
    "for combination, buildings in cluster_combinations.items():\n",
    "    if buildings:\n",
    "        print(f\"í´ëŸ¬ìŠ¤í„° {combination}: ë¹Œë”© {', '.join(map(str, buildings))}\")\n",
    "    else:\n",
    "        print(f\"í´ëŸ¬ìŠ¤í„° {combination}: ì—†ìŒ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘Œ í”„ë¡œí« ì‹¤í–‰ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ê³ , í•´ë‹¹ë˜ëŠ” ê±´ë¬¼ ë²ˆí˜¸ë§Œ ëª¨ì•„ì„œ ì €ì¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê° í´ëŸ¬ìŠ¤í„° ì¡°í•©ì— í•´ë‹¹í•˜ëŠ” ë¹Œë”© ë²ˆí˜¸\n",
    "cluster_combinations = {\n",
    "    ('1', 'a'): [3, 5, 15, 24, 26, 28, 32, 33, 42, 44, 52, 57, 60],\n",
    "    ('1', 'b'): [],\n",
    "    ('1', 'c'): [18, 37, 46, 47, 55],\n",
    "    ('1', 'd'): [11, 40],\n",
    "    ('2', 'a'): [],\n",
    "    ('2', 'b'): [1, 8, 30, 31, 38, 54],\n",
    "    ('2', 'c'): [],\n",
    "    ('2', 'd'): [],\n",
    "    ('3', 'a'): [],\n",
    "    ('3', 'b'): [],\n",
    "    ('3', 'c'): [2, 4, 6, 7, 9, 13, 14, 16, 17, 19, 20, 21, 22, 23, 25, 27, 29, 34, 35, 36, 39, 41, 43, 45, 48, 49, 50, 51, 53, 56, 58, 59],\n",
    "    ('3', 'd'): [10, 12]\n",
    "}\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ì¡°í•©ë³„ ë°ì´í„°ì…‹ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "clustered_dfs = {}\n",
    "\n",
    "# ê° í´ëŸ¬ìŠ¤í„° ì¡°í•©ì— ëŒ€í•´ ë°ì´í„°í”„ë ˆì„ í•„í„°ë§ ë° ì €ì¥\n",
    "for combination, buildings in cluster_combinations.items():\n",
    "    if buildings:\n",
    "        clustered_dfs[combination] = train_df[train_df['num'].isin(buildings)]\n",
    "    else:\n",
    "        clustered_dfs[combination] = pd.DataFrame(columns=train_df.columns)\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ ë°ì´í„°í”„ë ˆì„ì„ ë³€ìˆ˜ì— í• ë‹¹\n",
    "for combination, df in clustered_dfs.items():\n",
    "    var_name = f\"cluster_{combination[0]}_{combination[1]}\"\n",
    "    globals()[var_name] = df\n",
    "\n",
    "# í™•ì¸ì„ ìœ„í•´ ê° í´ëŸ¬ìŠ¤í„° ì¡°í•©ë³„ ë°ì´í„°í”„ë ˆì„ì˜ í¬ê¸° ì¶œë ¥\n",
    "for combination in cluster_combinations.keys():\n",
    "    var_name = f\"cluster_{combination[0]}_{combination[1]}\"\n",
    "    print(f\"{var_name}: {len(globals()[var_name])} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ê²°ê³¼ì²˜ëŸ¼ 12ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ„ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = cluster_1_aì— ëŒ€í•˜ì—¬ í”„ë¡œí«ì„ ì§„í–‰í•˜ë©´ ì´ë˜ì™€ ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ˜ êµ°ì§‘ë³„ë¡œ ê°ê° í”„ë¡œí« ëŒë ¤ë³¸ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬\n",
    "clusters = {\n",
    "    'cluster_1_a': cluster_1_a,\n",
    "    # 'cluster_1_b': cluster_1_b,\n",
    "    'cluster_1_c': cluster_1_c,\n",
    "    'cluster_1_d': cluster_1_d,\n",
    "    # 'cluster_2_a': cluster_2_a,\n",
    "    'cluster_2_b': cluster_2_b,\n",
    "    # 'cluster_2_c': cluster_2_c,\n",
    "    # 'cluster_2_d': cluster_2_d,\n",
    "    # 'cluster_3_a': cluster_3_a,\n",
    "    # 'cluster_3_b': cluster_3_b,\n",
    "    'cluster_3_c': cluster_3_c,\n",
    "    'cluster_3_d': cluster_3_d\n",
    "}\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ë¡œ Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì˜ˆì¸¡í•  ê¸°ê°„ ì„¤ì •, ì˜ˆ: 20ì¼)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "    fig1 = model.plot(forecast)\n",
    "    plt.title(f'{cluster_name} - Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    \n",
    "    # ê²€ì€ìƒ‰ ì  í¬ê¸° ì¤„ì´ê¸°\n",
    "    for line in fig1.gca().get_lines():\n",
    "        if line.get_marker() == 'o':\n",
    "            line.set_markersize(1)  # ì ì˜ í¬ê¸°ë¥¼ 1ë¡œ ì„¤ì •\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ˜ ê²°ê³¼ í•´ì„\n",
    "\n",
    "ì˜ˆì¸¡ ê·¸ë˜í”„ (`fig1`)\n",
    "\n",
    "1. **ê²€ì€ìƒ‰ ì  (ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸)**:\n",
    "   - ì´ ì ë“¤ì€ ì‹¤ì œ ì „ë ¥ ì†Œë¹„ ë°ì´í„°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. `cluster_1_a` ë°ì´í„°ì…‹ì˜ `target` ê°’ìœ¼ë¡œ, ì‹œê°„(`datetime`)ì— ë”°ë¥¸ ì‹¤ì œ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "2. **íŒŒë€ìƒ‰ ì„  (ì˜ˆì¸¡ ê°’)**:\n",
    "   - ì´ ì„ ì€ Prophet ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµí•œ í›„ ì˜ˆì¸¡í•œ ê°’ìœ¼ë¡œ, ì‹¤ì œ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **íŒŒë€ìƒ‰ ìŒì˜ ì˜ì—­ (ë¶ˆí™•ì‹¤ì„± ë²”ìœ„)**:\n",
    "   - ì´ ì˜ì—­ì€ ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„± ë²”ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Prophet ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ì˜ ì‹ ë¢° êµ¬ê°„ì„ ë‚˜íƒ€ë‚´ë©°, ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "4. **ë‚ ì§œ ì¶• (Xì¶•)**:\n",
    "   - Xì¶•ì€ ë‚ ì§œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆì¸¡ì´ ì´ë£¨ì–´ì§„ ì‹œê°„ ë²”ìœ„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "5. **ì „ë ¥ ì†Œë¹„ëŸ‰ ì¶• (Yì¶•)**:\n",
    "   - Yì¶•ì€ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. `target` ê°’ìœ¼ë¡œ, ì˜ˆì¸¡ëœ ì „ë ¥ ì†Œë¹„ëŸ‰ì˜ ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "6. **ì œëª© ë° ì¶• ë¼ë²¨**:\n",
    "   - ê·¸ë˜í”„ì˜ ì œëª©ì€ \"Electricity Consumption Forecast\"ë¡œ, ì „ë ¥ ì†Œë¹„ ì˜ˆì¸¡ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Xì¶• ë¼ë²¨ì€ \"Date\", Yì¶• ë¼ë²¨ì€ \"Electricity Consumption (target)\"ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì»´í¬ë„ŒíŠ¸ ê·¸ë˜í”„ (`fig2`)\n",
    "\n",
    "ì»´í¬ë„ŒíŠ¸ ê·¸ë˜í”„ëŠ” Prophet ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë” ìì„¸íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ëŠ” ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” ì£¼ìš” ìš”ì†Œë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "1. **ì¶”ì„¸ (Trend)**:\n",
    "   - ì „ë°˜ì ì¸ ì¶”ì„¸ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ ì†Œë¹„ì˜ ì¥ê¸°ì ì¸ ë³€í™” íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì£¼ê¸°ì„± (Yearly, Weekly, Daily Seasonality)**:\n",
    "   - íŠ¹ì • ì£¼ê¸°(ì—°ê°„, ì£¼ê°„, ì¼ê°„)ë§ˆë‹¤ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì „ë ¥ ì†Œë¹„ê°€ íŠ¹ì • ì‹œê°„ëŒ€ë‚˜ ìš”ì¼ì— ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "3. **ê¸°íƒ€ ìš”ì†Œ (Holidays, Special Events)**:\n",
    "   - ê³µíœ´ì¼ì´ë‚˜ íŠ¹ë³„í•œ ì´ë²¤íŠ¸ì™€ ê°™ì€ íŠ¹ì • ì´ë²¤íŠ¸ì— ë”°ë¥¸ ë³€í™”ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” íŠ¹ë³„íˆ ì¶”ê°€ëœ ì´ë²¤íŠ¸ê°€ ì—†ë‹¤ë©´ í‘œì‹œë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°ë¡ \n",
    "\n",
    "ì´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í†µí•´ ì „ë ¥ ì†Œë¹„ íŒ¨í„´ì„ ì´í•´í•˜ê³ , ë¯¸ë˜ì˜ ì „ë ¥ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸(ê²€ì€ìƒ‰ ì )ì™€ ëª¨ë¸ ì˜ˆì¸¡ ê°’(íŒŒë€ìƒ‰ ì„ )ì„ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìœ¼ë©°, ë¶ˆí™•ì‹¤ì„± ë²”ìœ„(íŒŒë€ìƒ‰ ìŒì˜)ë¥¼ í†µí•´ ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ğŸ˜ 1~60ë²ˆ ê±´ë¬¼ì„ ì „ë¶€ ê°ê° í”„ë¡œí« ëŒë ¤ë³¸ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ìœ í•œ num ê°’ì„ ì¶”ì¶œ\n",
    "unique_nums = train_df['num'].unique()\n",
    "\n",
    "# ê° num ê°’ì— ëŒ€í•´ ë°ì´í„°í”„ë ˆì„ì„ í•„í„°ë§í•˜ê³  ë³€ìˆ˜ì— í• ë‹¹\n",
    "for num in unique_nums:\n",
    "    globals()[f'num_{num}'] = train_df[train_df['num'] == num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "clusters = {f'num_{i}': globals()[f'num_{i}'] for i in range(1, 61)}\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ë¡œ Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì˜ˆì¸¡í•  ê¸°ê°„ ì„¤ì •, ì˜ˆ: 20ì¼)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "    fig1 = model.plot(forecast)\n",
    "    plt.title(f'{cluster_name} - Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    \n",
    "    # ê²€ì€ìƒ‰ ì  í¬ê¸° ì¤„ì´ê¸°\n",
    "    for line in fig1.gca().get_lines():\n",
    "        if line.get_marker() == 'o':\n",
    "            line.set_markersize(1)  # ì ì˜ í¬ê¸°ë¥¼ 1ë¡œ ì„¤ì •\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ëŠ” Prophet ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ë¥¼ RMSEê°’ì„ êµ¬í•˜ì—¬ ë¹„êµí–ˆë‹¤. RMSEê°’ì´ ê°€ì¥ ë‚®ì€ ê±´ë¬¼ì€ 32ë²ˆ (18.9)ì´ê³ , ê°€ì¥ ë†’ì€ ê±´ë¬¼ì€ 8ë²ˆ(1945.2)ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ì§€í‘œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "performance_metrics = []\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ë¡œ Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "for cluster_name, df in clusters.items():\n",
    "    if df.empty:\n",
    "        print(f\"{cluster_name}: No data available\")\n",
    "        continue\n",
    "    \n",
    "    # Prophet ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "    df = df.rename(columns={'datetime': 'ds', 'target': 'y'})\n",
    "    \n",
    "    # Prophet ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì˜ˆì¸¡í•  ê¸°ê°„ ì„¤ì •, ì˜ˆ: 20ì¼)\n",
    "    future = model.make_future_dataframe(periods=20)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì˜ˆ: MAE, RMSE)\n",
    "    y_true = df['y'].values\n",
    "    y_pred = forecast['yhat'][:len(y_true)].values  # ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ ì‹¤ì œ ë°ì´í„° ê¸¸ì´ë§Œí¼ ì‚¬ìš©\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    performance_metrics.append({\n",
    "        'Building': cluster_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "\n",
    "# ì„±ëŠ¥ ì§€í‘œë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "performance_df = pd.DataFrame(performance_metrics)\n",
    "\n",
    "# RMSE ê°’ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì •ë ¬\n",
    "performance_df = performance_df.sort_values(by='RMSE')\n",
    "\n",
    "# ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥\n",
    "print(performance_df)\n",
    "\n",
    "# ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™” (RMSEë§Œ)\n",
    "performance_df.plot(x='Building', y='RMSE', kind='bar', figsize=(15, 7), legend=False)\n",
    "plt.title('Prediction RMSE by Building (Sorted)')\n",
    "plt.xlabel('Building')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘Œ LSTM ëŒë ¤ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸš 1~60ë²ˆ ê±´ë¬¼ ëª¨ë‘ ëŒë ¤ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ìœ í•œ num ê°’ì„ ì¶”ì¶œ\n",
    "unique_nums = train_df['num'].unique()\n",
    "\n",
    "# ê° num ê°’ì— ëŒ€í•´ ë°ì´í„°í”„ë ˆì„ì„ í•„í„°ë§í•˜ê³  ë³€ìˆ˜ì— í• ë‹¹\n",
    "for num in unique_nums:\n",
    "    globals()[f'num_{num}'] = train_df[train_df['num'] == num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "df = num_1.copy()\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "# LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "sequence_length = 20\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "    y_train.append(scaled_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¬êµ¬ì„±\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# LSTM ëª¨ë¸ ìƒì„±\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# ë¯¸ë˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì¤€ë¹„\n",
    "test_data = scaled_data[-sequence_length:]\n",
    "x_test = [test_data]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "predicted_values = []\n",
    "\n",
    "for _ in range(200):\n",
    "    prediction = model.predict(x_test)\n",
    "    predicted_values.append(prediction[0])\n",
    "    new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "    x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['target'], label='Actual')\n",
    "plt.plot(pd.date_range(start=df.index[-1], periods=201, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "plt.title('1A Electricity Consumption Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Consumption (target)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "df = num_60.copy()\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "# LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "sequence_length = 20\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "    y_train.append(scaled_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¬êµ¬ì„±\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# LSTM ëª¨ë¸ ìƒì„±\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# ë¯¸ë˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì¤€ë¹„\n",
    "test_data = scaled_data[-sequence_length:]\n",
    "x_test = [test_data]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "predicted_values = []\n",
    "\n",
    "for _ in range(200):\n",
    "    prediction = model.predict(x_test)\n",
    "    predicted_values.append(prediction[0])\n",
    "    new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "    x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['target'], label='Actual')\n",
    "plt.plot(pd.date_range(start=df.index[-1], periods=201, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "plt.title('Electricity Consumption Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Consumption (target)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "clusters = {f'num_{i}': globals()[f'num_{i}'].copy() for i in range(1, 61)}\n",
    "\n",
    "# LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# í•™ìŠµ ë° ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n",
    "def train_and_predict(df, sequence_length=20, epochs=50, batch_size=32, future_steps=200):\n",
    "    # ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, len(scaled_data)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # ë¯¸ë˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì¤€ë¹„\n",
    "    test_data = scaled_data[-sequence_length:]\n",
    "    x_test = [test_data]\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    predicted_values = []\n",
    "    for _ in range(future_steps):\n",
    "        prediction = model.predict(x_test)\n",
    "        predicted_values.append(prediction[0])\n",
    "        new_test_data = np.append(x_test[0][1:], prediction[0])\n",
    "        x_test = np.reshape(new_test_data, (1, sequence_length, 1))\n",
    "\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df.index, df['target'], label='Actual')\n",
    "    plt.plot(pd.date_range(start=df.index[-1], periods=future_steps + 1, freq='H')[1:], predicted_values, label='Predicted', color='red')\n",
    "    plt.title(f'{df.name} Electricity Consumption Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Electricity Consumption (target)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "for cluster_name, cluster_df in clusters.items():\n",
    "    cluster_df.name = cluster_name\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    train_and_predict(cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# í•™ìŠµ ë° ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_data) * 0.8)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "    test_data = scaled_data[int(len(scaled_data) * 0.8) - sequence_length:]\n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data)):\n",
    "        x_test.append(test_data[i-sequence_length:i, 0])\n",
    "        y_test.append(test_data[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # RMSE ê³„ì‚°\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ RMSE ê³„ì‚°\n",
    "rmses = {}\n",
    "for i in range(1, 61):  # ì˜ˆì œì—ì„œëŠ” num_1ë¶€í„° num_3ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” (1, 61)ë¡œ ë³€ê²½í•˜ì„¸ìš”.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    rmse = train_and_evaluate(df)\n",
    "    rmses[cluster_name] = rmse\n",
    "    print(f'Building {cluster_name} - RMSE: {rmse}')\n",
    "\n",
    "# RMSE ê°’ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "sorted_rmses = dict(sorted(rmses.items(), key=lambda item: item[1]))\n",
    "\n",
    "# RMSE ê°’ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì¶œë ¥\n",
    "rmse_df = pd.DataFrame(list(sorted_rmses.items()), columns=['Cluster', 'RMSE'])\n",
    "print(rmse_df)\n",
    "\n",
    "# RMSE ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(rmse_df['Cluster'], rmse_df['RMSE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ë³€ëŸ‰ìœ¼ë¡œ ëŒë ¸ì„ ë•Œ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# SMAPE ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# í•™ìŠµ ë° ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df['target'].values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_data) * 0.8)):\n",
    "        x_train.append(scaled_data[i-sequence_length:i, 0])\n",
    "        y_train.append(scaled_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # LSTM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = create_lstm_model((x_train.shape[1], 1))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "    test_data = scaled_data[int(len(scaled_data) * 0.8) - sequence_length:]\n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data)):\n",
    "        x_test.append(test_data[i-sequence_length:i, 0])\n",
    "        y_test.append(test_data[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # SMAPE ê³„ì‚°\n",
    "    smape_value = smape(y_test, predictions)\n",
    "    \n",
    "    return smape_value\n",
    "\n",
    "# ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ SMAPE ê³„ì‚°\n",
    "smapes = {}\n",
    "for i in range(1, 61):  # ì˜ˆì œì—ì„œëŠ” num_1ë¶€í„° num_60ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    smape_value = train_and_evaluate(df)\n",
    "    smapes[cluster_name] = smape_value\n",
    "    print(f'Building {cluster_name} - SMAPE: {smape_value}')\n",
    "\n",
    "# SMAPE ê°’ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "sorted_smapes = dict(sorted(smapes.items(), key=lambda item: item[1]))\n",
    "\n",
    "# SMAPE ê°’ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì¶œë ¥\n",
    "smape_df = pd.DataFrame(list(sorted_smapes.items()), columns=['Cluster', 'SMAPE'])\n",
    "print(smape_df)\n",
    "\n",
    "# SMAPE ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(smape_df['Cluster'], smape_df['SMAPE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.title('SMAPE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë³€ëŸ‰ìœ¼ë¡œ ëŒë ¸ì„ ë•Œ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# SMAPE ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# í•™ìŠµ ë° ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n",
    "def train_and_evaluate(df, sequence_length=20, epochs=50, batch_size=32):\n",
    "    # ë°ì´í„°í”„ë ˆì„ì„ 'datetime'ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬\n",
    "    features = df.drop(columns=['target'])\n",
    "    target = df['target']\n",
    "\n",
    "    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    # LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(sequence_length, int(len(scaled_target) * 0.8)):\n",
    "        x_train.append(scaled_features[i-sequence_length:i, :])\n",
    "        y_train.append(scaled_target[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "    # LSTM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = create_lstm_model((x_train.shape[1], x_train.shape[2]))\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "    test_data_features = scaled_features[int(len(scaled_target) * 0.8) - sequence_length:]\n",
    "    test_data_target = scaled_target[int(len(scaled_target) * 0.8) - sequence_length:]\n",
    "    \n",
    "    x_test, y_test = [], []\n",
    "    for i in range(sequence_length, len(test_data_target)):\n",
    "        x_test.append(test_data_features[i-sequence_length:i, :])\n",
    "        y_test.append(test_data_target[i, 0])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(np.column_stack([np.zeros((predictions.shape[0], scaled_features.shape[1]-1)), predictions]))[:, -1]\n",
    "    y_test = scaler.inverse_transform(np.column_stack([np.zeros((y_test.shape[0], scaled_features.shape[1]-1)), y_test.reshape(-1, 1)]))[:, -1]\n",
    "\n",
    "    # SMAPE ê³„ì‚°\n",
    "    smape_value = smape(y_test, predictions)\n",
    "    \n",
    "    return smape_value\n",
    "\n",
    "# ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ SMAPE ê³„ì‚°\n",
    "smapes = {}\n",
    "for i in range(1, 61):  # ì˜ˆì œì—ì„œëŠ” num_1ë¶€í„° num_60ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    cluster_name = f'num_{i}'\n",
    "    print(f'Processing {cluster_name}...')\n",
    "    df = globals()[cluster_name].copy()\n",
    "    smape_value = train_and_evaluate(df)\n",
    "    smapes[cluster_name] = smape_value\n",
    "    print(f'Building {cluster_name} - SMAPE: {smape_value}')\n",
    "\n",
    "# SMAPE ê°’ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "sorted_smapes = dict(sorted(smapes.items(), key=lambda item: item[1]))\n",
    "\n",
    "# SMAPE ê°’ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì¶œë ¥\n",
    "smape_df = pd.DataFrame(list(sorted_smapes.items()), columns=['Cluster', 'SMAPE'])\n",
    "print(smape_df)\n",
    "\n",
    "# SMAPE ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(smape_df['Cluster'], smape_df['SMAPE'])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.title('SMAPE of LSTM Model for Each Cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ğŸ‘Œí´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì•„ë¦¬ë§ˆ ëŒë ¤ë³¸ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_1_a['datetime'] = pd.to_datetime(cluster_1_a['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_1_a['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_a[cluster_1_a['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_1_c['datetime'] = pd.to_datetime(cluster_1_c['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_1_c['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_c[cluster_1_c['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_1_d['datetime'] = pd.to_datetime(cluster_1_d['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_1_d['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_1_d[cluster_1_d['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_2_b['datetime'] = pd.to_datetime(cluster_2_b['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_2_b['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_2_b[cluster_2_b['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_3_c['datetime'] = pd.to_datetime(cluster_3_c['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_3_c['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_3_c[cluster_3_c['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datetime' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "cluster_3_d['datetime'] = pd.to_datetime(cluster_3_d['datetime'])\n",
    "\n",
    "# unique 'num' ê°’ë“¤\n",
    "unique_nums = cluster_3_d['num'].unique()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for num in unique_nums:\n",
    "    subset = cluster_3_d[cluster_3_d['num'] == num]\n",
    "    plt.plot(subset['datetime'], subset['target'], label=f'num {num}')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target over Time for Each num')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼í”¼í„°ì—ì„œ ëŒë¦¬ê¸° ì–´ë ¤ìš°ë‹ˆ ì½”ë©ì—ì„œ ëŒë¦¬ëŠ” ê²ƒì„ ê¶Œì¥!\n",
    "# í•˜ë‹¨ ì£¼ì„ ì²˜ë¦¬ëŠ” Pycaretì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. Pycaret ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ í•„ìš”!\n",
    "\n",
    "!pip install pycaret\n",
    "import os\n",
    "\n",
    "# ì˜¤ì§ ì¤‘ìš”í•œ ë¡œê¹…ë§Œ í™œì„±í™”í•˜ê¸° (ì„ íƒì )\n",
    "os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "\n",
    "# ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸\n",
    "def what_is_installed():\n",
    "    from pycaret import show_versions\n",
    "    show_versions()\n",
    "\n",
    "try:\n",
    "    what_is_installed()\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pycaret\n",
    "    what_is_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "!pip install sktime\n",
    "\n",
    "# data analysis and wrangling\n",
    "import sys\n",
    "import sktime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import scipy.stats\n",
    "import tqdm as tq\n",
    "# import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import missingno as msno\n",
    "\n",
    "# EDA & visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm           # íšŒê·€ / ì‹œê³„ì—´ ... ì¼ë°˜ì ì¸ ëª¨ë¸\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor # ë‹¤ì¤‘ê³µì„ ì„±\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Time series\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "# ML Algorithms\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# model validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "\n",
    "# deep-learning libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, GRU, RNN\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# ê²½ê³  ë¬´ì‹œ\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")\n",
    "print(f\"joblib version: {joblib.__version__}\")\n",
    "print(f\"missingno version: {msno.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"sktime version: {sktime.__version__}\")\n",
    "print(f\"scikit-learn version: {skl.__version__}\")\n",
    "# print(f\"lightgbm version: {lgb.__version__}\")\n",
    "# print(f\"catboost version: {catboost.__version__}\")\n",
    "print(f\"tqdm version: {tq.__version__}\")\n",
    "# print(f\"xgboost version: {xgb.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "print(\"------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\train.csv', encoding='cp949')\n",
    "test_df = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\test.csv', encoding='cp949')\n",
    "sub = pd.read_csv(r'C:\\Users\\USER\\projects\\data_study\\sample_submission.csv', encoding='cp949')\n",
    "\n",
    "# renaming columns\n",
    "train_df.columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "test_df.columns = ['num','datetime','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
    "\n",
    "# ë°ì´í„° í˜•íƒœ ë° ê°œìˆ˜, ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100\n",
    "\n",
    "def datetimeTransform(data):\n",
    "    data.insert(0, \"date_time\", pd.to_datetime(data[\"datetime\"]))\n",
    "    data.drop(columns=[\"datetime\"], inplace=True)\n",
    "    return data.head(25)\n",
    "\n",
    "df_dict = {}\n",
    "def separateDataFrame(data):\n",
    "    for i in range(1, 61):\n",
    "        df_dict[i] = data[data['num'] == i]\n",
    "    return df_dict\n",
    "\n",
    "def CDH(xs):\n",
    "    ys = []\n",
    "    for i in range(len(xs)):\n",
    "        if i < 11:\n",
    "            ys.append(np.sum(xs[:(i+1)]-26))\n",
    "        else:\n",
    "            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "    return np.array(ys)\n",
    "\n",
    "def THI(data) :\n",
    "    data['THI'] = round(1.8*data['temperature'] - 0.55*(1-(data['humidity']/100))*(1.8*data['temperature']-26) + 32, 2)\n",
    "    # print(data['THI'].head())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df -> ê±´ë¬¼ë²ˆí˜¸ ë³„ ë°ì´í„°í”„ë ˆì„ ì •ë ¬\n",
    "\n",
    "# ëƒ‰ë°©ë„ì‹œê°„ CDH(Cooling Degree Hour)\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = train_df[train_df['num'] == num]\n",
    "    cdh = CDH(temp['temperature'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "train_df['CDH'] = cdhs\n",
    "\n",
    "# ë¶ˆì¾Œì§€ìˆ˜ í•¨ìˆ˜\n",
    "THI(train_df)\n",
    "# íŒë‹¤ìŠ¤ datetime ë³€í™˜\n",
    "datetimeTransform(train_df)\n",
    "# ë¹„ì „ê¸°ëƒ‰ë°©, íƒœì–‘ê´‘ ë³€ìˆ˜ ì‚­ì œ\n",
    "# 60ê°œ ê°ê°ì˜ ë™ì¼ ê±´ë¬¼ë³„ ëª¨ë¸ë§ ì‹œ ë¹„ì „ê¸°ëƒ‰ë°©ê³¼ íƒœì–‘ê´‘ ì—¬ë¶€ëŠ” í•„ìš”ì—†ìŒ -> ì‚­ì œ\n",
    "train_df.drop(columns=['nelec_cool_flag','solar_flag'], inplace=True)\n",
    "# ê±´ë¬¼ë²ˆí˜¸ êµ¬ë¶„ ë° ë°ì´í„° ì €ì¥ í•¨ìˆ˜\n",
    "separateDataFrame(train_df)\n",
    "df_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¼ì¡°\n",
    "# discreteí•œ ë¶„í¬ë¡œ ê°€ì¥ ìµœê·¼ì— ê¸°ë¡ëœ ì¼ì¡°ê°’ì„ ê°€ì ¸ì˜¤ëŠ” í˜•íƒœë¡œ ê²°ì¸¡ì¹˜ ë³´ê°„\n",
    "train_df['insolation'].value_counts()\n",
    "test_df['insolation'] = test_df['insolation'].interpolate(method='pad')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°: ê¸°ì˜¨, í’ì†, ìŠµë„, ê°•ìˆ˜ëŸ‰\n",
    "# ê¸°ì˜¨, í’ì†, ìŠµë„, ê°•ìˆ˜ëŸ‰ì€ 'pad','linear','quadratic','cubic' ë°©ë²• ì¤‘\n",
    "# train ë°ì´í„°ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ë³´ê°„ë²•ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ë³´ê°„ ì‹œí–‰\n",
    "# ìš°ì„  train ë°ì´í„° ì„ì˜ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ìƒì„±\n",
    "train_ = train_df.copy(deep=True)\n",
    "def make_train_nan(col, n):\n",
    "    new_list = []\n",
    "    for idx, temp in enumerate(train_[col]):\n",
    "        if idx%n==0:\n",
    "            new_list.append(temp)\n",
    "        else:\n",
    "            new_list.append(np.nan)\n",
    "    train_['{}'.format(col+'_nan')] = new_list\n",
    "make_train_nan('temperature',3)\n",
    "make_train_nan('windspeed',3)\n",
    "make_train_nan('humidity',3)\n",
    "make_train_nan('precipitation',6)\n",
    "print(train_.iloc[:,-4:].isnull().sum())\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ê°€ ì˜ ìƒì„±ë¨ì„ í™•ì¸í•˜ì˜€ìœ¼ë©° ê° ë³€ìˆ˜ì— ëŒ€í•´ 4ê°€ì§€ì˜ ë³´ê°„ë²•ì„ ì‹œí–‰í•œ í›„\n",
    "# ê°€ì¥ ì˜¤ì°¨ê°€ ì ì€ ë³´ê°„ë²•ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ë³´ê°„\n",
    "def compare_interpolate_methods(col, methods, metric):\n",
    "    error_dict = dict()\n",
    "    for method in methods:\n",
    "        fillna = train_['{}'.format(col+'_nan')].interpolate(method=method)\n",
    "        if fillna.isna().sum() != 0:\n",
    "            fillna = fillna.interpolate(method='linear')\n",
    "        error = metric(train_['{}'.format(col)], fillna)\n",
    "        error_dict['{}'.format(method)] = error\n",
    "\n",
    "    return error_dict\n",
    "all_cols_error_dict = dict()\n",
    "for col in ['temperature', 'windspeed', 'humidity', 'precipitation']:\n",
    "    methods = ['pad','linear','quadratic','cubic']\n",
    "    error_dict = compare_interpolate_methods(col, methods, mean_squared_error)\n",
    "    all_cols_error_dict['{}'.format(col)] = error_dict\n",
    "\n",
    "all_cols_error_df = pd.DataFrame(all_cols_error_dict)\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize = (18,5), sharey=False)\n",
    "for i in range(len(all_cols_error_df.columns)):\n",
    "    sns.lineplot(ax=axes[i], data=all_cols_error_df.iloc[:,i].transpose())\n",
    "\n",
    "#ê¸°ì˜¨ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "test_df['temperature'] = test_df['temperature'].interpolate(method='quadratic')\n",
    "#ë§ˆì§€ë§‰ na ì±„ìš°ê¸°\n",
    "test_df['temperature'] = test_df['temperature'].interpolate(method='linear')\n",
    "\n",
    "#í’ì† ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "test_df['windspeed'] = test_df['windspeed'].interpolate(method='linear')\n",
    "\n",
    "#ìŠµë„ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "test_df['humidity'] = test_df['humidity'].interpolate(method='quadratic')\n",
    "#ë§ˆì§€ë§‰ na ì±„ìš°ê¸°\n",
    "test_df['humidity'] = test_df['humidity'].interpolate(method='linear')\n",
    "\n",
    "#ê°•ìˆ˜ëŸ‰ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "test_df['precipitation'] = test_df['precipitation'].interpolate(method='linear')\n",
    "\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df -> ê±´ë¬¼ë²ˆí˜¸ ë³„ ë°ì´í„°í”„ë ˆì„ ì •ë ¬\n",
    "\n",
    "# # ê²°ì¸¡ê°’ ì²˜ë¦¬ pandas ë‚´ ì„ í˜•ë³´ê°„ method ì‚¬ìš©\n",
    "# for i in range(60):\n",
    "#     test_df.iloc[i*168:(i+1)*168, :]  = test_df.iloc[i*168:(i+1)*168, :].interpolate()\n",
    "\n",
    "# ëƒ‰ë°©ë„ì‹œê°„ CDH(Cooling Degree Hour)\n",
    "cdhs = np.array([])\n",
    "for num in range(1,61,1):\n",
    "    temp = test_df[test_df['num'] == num]\n",
    "    cdh = CDH(temp['temperature'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "test_df['CDH'] = cdhs\n",
    "\n",
    "# ë¶ˆì¾Œì§€ìˆ˜ í•¨ìˆ˜\n",
    "THI(test_df)\n",
    "# íŒë‹¤ìŠ¤ datetime ë³€í™˜\n",
    "datetimeTransform(test_df)\n",
    "# ë¹„ì „ê¸°ëƒ‰ë°©, íƒœì–‘ê´‘ ë³€ìˆ˜ ì‚­ì œ\n",
    "# 60ê°œ ê°ê°ì˜ ë™ì¼ ê±´ë¬¼ë³„ ëª¨ë¸ë§ ì‹œ ë¹„ì „ê¸°ëƒ‰ë°©ê³¼ íƒœì–‘ê´‘ ì—¬ë¶€ëŠ” í•„ìš”ì—†ìŒ -> ì‚­ì œ\n",
    "test_df.drop(columns=['nelec_cool_flag','solar_flag'], inplace=True)\n",
    "# ê±´ë¬¼ë²ˆí˜¸ êµ¬ë¶„ ë° ë°ì´í„° ì €ì¥ í•¨ìˆ˜\n",
    "test_df_dict = {}\n",
    "for i in range(1, 61):\n",
    "    test_df_dict[i] = test_df[test_df['num'] == i]\n",
    "test_df_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê±´ë¬¼ ë³„ train_df test_df ì˜ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì‚­ì œí•¨\n",
    "for i in range(1,61):\n",
    "    df_dict[i].reset_index(drop=True, inplace=True)\n",
    "    df_dict[i].drop(columns=['num'], inplace=True)\n",
    "    test_df_dict[i].reset_index(drop=True, inplace=True)\n",
    "    test_df_dict[i].drop(columns=['num'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numPlot(data,i:int):\n",
    "    ''' ê±´ë¬¼ë²ˆí˜¸ë¡œ ë¶„ë¥˜í•œ ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ì‚¬ìš©ëŸ‰ í•¨ìˆ˜'''\n",
    "    num_i = data[i].copy(deep=True)\n",
    "\n",
    "    num_i.set_index('date_time',inplace=True)\n",
    "    num_i.index = pd.to_datetime(num_i.index)\n",
    "    return num_i.loc[:,'target']\n",
    "numPlot(df_dict,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pycaret.anomaly import *\n",
    "\n",
    "# Assuming df_dict is already defined and contains your data\n",
    "df_dict1 = {}\n",
    "anomaly_counts = {}\n",
    "interpolated_counts = {}\n",
    "\n",
    "# Create subplots for anomaly detection results\n",
    "fig_anomaly = make_subplots(rows=15, cols=4, subplot_titles=[f'Building {i}' for i in range(1, 61)])\n",
    "# Create subplots for interpolated data results\n",
    "fig_interpolated = make_subplots(rows=15, cols=4, subplot_titles=[f'Building {i}' for i in range(1, 61)])\n",
    "\n",
    "for i in range(1, 61):\n",
    "    data = df_dict[i].copy(deep=True)\n",
    "\n",
    "    # set timestamp to index\n",
    "    data.set_index('date_time', drop=True, inplace=True)\n",
    "\n",
    "    # resample timeseries to hourly\n",
    "    data = data.resample('H').sum()\n",
    "\n",
    "    # create features from date\n",
    "    data['day'] = [i.day for i in data.index]\n",
    "    data['day_name'] = [i.day_name() for i in data.index]\n",
    "    data['day_of_year'] = [i.dayofyear for i in data.index]\n",
    "    data['week_of_year'] = [i.isocalendar()[1] for i in data.index]\n",
    "    data['hour'] = [i.hour for i in data.index]\n",
    "    data['is_weekday'] = [i.isoweekday() for i in data.index]\n",
    "\n",
    "    # init setup\n",
    "    s = setup(data, session_id=42,\n",
    "              ordinal_features={'day_name': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday', 'Saturday']},\n",
    "              numeric_features=['is_weekday'])\n",
    "\n",
    "    # train model\n",
    "    lof = create_model('lof')\n",
    "    lof_results = assign_model(lof)\n",
    "\n",
    "    # count anomalies\n",
    "    anomaly_count = lof_results['Anomaly'].sum()\n",
    "    anomaly_counts[i] = anomaly_count\n",
    "\n",
    "    # filter out anomalies\n",
    "    filtered_data = lof_results[lof_results['Anomaly'] != 1]\n",
    "\n",
    "    # interpolate missing values\n",
    "    interpolated_data = filtered_data.copy()\n",
    "    interpolated_data['target'] = interpolated_data['target'].interpolate()\n",
    "\n",
    "    # count interpolated values\n",
    "    interpolated_count = interpolated_data['target'].isna().sum()\n",
    "    interpolated_counts[i] = interpolated_count\n",
    "\n",
    "    # Add anomaly detection plot to subplot\n",
    "    outlier_dates = lof_results[lof_results['Anomaly'] == 1].index\n",
    "    y_values = [lof_results.loc[j]['target'] for j in outlier_dates]\n",
    "    row = (i-1) // 4 + 1\n",
    "    col = (i-1) % 4 + 1\n",
    "    fig_anomaly.add_trace(go.Scatter(x=lof_results.index, y=lof_results['target'], mode='lines', name=f'Building {i}'),\n",
    "                          row=row, col=col)\n",
    "    fig_anomaly.add_trace(go.Scatter(x=outlier_dates, y=y_values, mode='markers', name='Anomaly', marker=dict(color='red', size=5)),\n",
    "                          row=row, col=col)\n",
    "\n",
    "    # Add interpolated data plot to subplot\n",
    "    fig_interpolated.add_trace(go.Scatter(x=interpolated_data.index, y=interpolated_data['target'], mode='lines', name=f'Building {i}'),\n",
    "                               row=row, col=col)\n",
    "\n",
    "    df_dict1[i] = lof_results\n",
    "    df_dict1[i].drop(columns=['day', 'day_name', 'day_of_year', 'week_of_year', 'hour', 'is_weekday', 'Anomaly', 'Anomaly_Score'], inplace=True)\n",
    "    df_dict1[i].reset_index(inplace=True)\n",
    "    print(df_dict1[i])\n",
    "\n",
    "# Update layout and show anomaly detection subplot\n",
    "fig_anomaly.update_layout(height=3000, width=1200, title_text='ì „ë ¥ì‚¬ìš©ëŸ‰ - UNSUPERVISED ANOMALY DETECTION', showlegend=False)\n",
    "fig_anomaly.show()\n",
    "\n",
    "# Update layout and show interpolated data subplot\n",
    "fig_interpolated.update_layout(height=3000, width=1200, title_text='ì „ë ¥ì‚¬ìš©ëŸ‰ - Interpolated Data (Outliers Removed)', showlegend=False)\n",
    "fig_interpolated.show()\n",
    "\n",
    "# Print anomaly and interpolation counts\n",
    "for i in range(1, 61):\n",
    "    print(f\"Building {i}: {anomaly_counts[i]} anomalies, {interpolated_counts[i]} interpolations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4ì¡°_ë¶„ì„ì½”ë“œ(2ì°¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
